\documentclass[10pt]{article}
\usepackage{color}
\usepackage{graphicx} % Required for inserting
\usepackage{booktabs} % For better-quality horizontal lines
\usepackage{multirow}
\usepackage{threeparttable} % For table notes
\usepackage{array} % For table formatting
\usepackage{setspace}
\usepackage{float} % add this in your preamble
\usepackage{amsmath}
\usepackage{tabularx}

\newcommand{\ed}[1]{\textcolor{red}{[EI:#1]}}
\newcommand{\yize}[1]{\textcolor{green}{[YH:#1]}}
\newcommand{\aaron}[1]{\textcolor{blue}{[AA:#1]}}

\usepackage[round]{natbib}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage[font={small,it}]{caption}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{graphicx, fullpage, verbatim, amsmath}
\usepackage{url, amsfonts, amssymb, amsthm,color, enumerate}
\usepackage{amsfonts}
\usepackage{bm}

\doublespacing
% \usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{graphicx}
\bibliographystyle{apalike}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\newcommand\choleraDeath{\delta_{C}}

% Using \doublespacing in the preamble 
% changes the text to double-line spacing
\doublespacing


\begin{document}

\title{The Poisson approximate likelihood compared to the particle filter}
\author{Yize Hao, Aaron A. Abkemeier and Edward L. Ionides}
\date{Draft compiled on \today}
\maketitle

\begin{abstract}
Filtering algorithms are fundamental for inference on partially observed stochastic dynamic systems, since they provide access to the likelihood function and hence enable likelihood-based or Bayesian inference.
Recently, a novel Poisson approximate likelihood (PAL) filter was introduced by \cite{wwr}.
PAL employs a Poisson approximation to conditional densities, offering a fast and consistent approximation to the likelihood function for a certain subset of partially observed Markov process models.
\cite{wwr} supported the importance of their contribution by theoretical analysis and numerical results.
A central piece of evidence for PAL is the comparison in Table~1 of \cite{wwr}, which claims a large improvement for PAL over a standard particle filter algorithm.
This evidence, based on a model and data from a previous scientific study, suggests at face value that researchers confronted with similar models would be irresponsible not to adopt PAL.
We show that the evidence is flawed because the PAL calculations were carried out using a dataset differently scaled from the previous study.
If PAL and the particle filter are used on this new scale, the superficial advantage of PAL largely disappears.
On simulations where the model is correctly specified, the particle filter outperforms PAL.
If a poorly fitting specification of initial value parameters is ammended, the particle filter also outperforms PAL on the actual data.
\end{abstract}


% R starts here

<<setup,echo=FALSE,message=FALSE,warning=FALSE>>=
rm(list = ls())     # clear objects

library("knitr")
opts_knit$set(concordance=TRUE)
opts_chunk$set(
#    concordance = TRUE,
    tidy = TRUE,
#    message = FALSE,
#    warning = FALSE,
    tidy.opts = list(
        keep.blank.line = FALSE
    ),
    comment = "",
    echo = FALSE,
    results = FALSE,
    dev.args = list(
        bg = "transparent",
        pointsize = 9
    ),
    fig.path = "figure/"
)

myround <- function(x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

graphics.off()      # close graphics windows
library(pomp)
library(magrittr)
library(ggplot2)
library(scales)
library(foreach)
library(doParallel)
library(doFuture)
library(dplyr)
library(Rcpp)
library(RcppArmadillo)
# install.packages('RcppDist')
sourceCpp('rotavirus_normq.cpp')
source("choose_n_fits.R")
source("eval_logLik_pomp.R")

RUN_LEVEL = 1
NP_MIF       = switch(RUN_LEVEL, 10, 50000)
NMIF         = switch(RUN_LEVEL, 4,    100)
ncores       = switch(RUN_LEVEL, 2,    36)
top_n_fits   = switch(RUN_LEVEL, 1,    12)
sim_rep      = switch(RUN_LEVEL, 1,    100) # Total number of simulations

#population number
pop <- 82372825

out_dir <- paste0("output_0",RUN_LEVEL, "/")
@


% OvOv model is here

<<OvOv-model>>=
# measurement model 
dmeas <- Csnippet("
                  if (ISNA(cases1)) {
                  lik = (give_log) ? 0 : 1;
                  } else {
                        lik =  dbinom(cases1, H1, q1, 1) +
                        dbinom(cases2, H2, q2, 1) +
                        dbinom(cases3, H3, q3, 1);
                      
                    lik = (give_log) ? lik : exp(lik);
                        
                    }")
rmeas <-  Csnippet("
                    cases1 = rbinom(H1, q1);
                    cases2 = rbinom(H2, q2);
                    cases3 = rbinom(H3, q3);
                  ")


rproc <- Csnippet("
    int I = I1 + I2 + I3;
    int trans_S1[3], trans_S2[3], trans_S3[2], trans_I1[3], trans_I2[3], trans_I3[2], trans_R1[3], trans_R2[3], trans_R3[2];
    
    double prob_S1[3],prob_I1[3],prob_R1[3],prob_S2[3],prob_I2[3],prob_R2[3],prob_S3[2],prob_I3[2],prob_R3[2];
    
    double xi = rgamma(sigma_xi, 1/sigma_xi);
    
    double kappa = (1 + beta11*cos(2*3.141593*t/52 + phi)) * xi;
    
    // Define rate
    prob_S1[0] = 1-exp(-dt*beta1*kappa*I/N); // 0->1
    prob_S1[1] = 1-exp(-delta1*dt);
    prob_S1[2] = exp(-delta1*dt) + exp(-dt*beta1*kappa*I/N) - 1;
    
    prob_I1[0] = 1-exp(-gamma*dt);
    prob_I1[1] = 1-exp(-delta1*dt);
    prob_I1[2] = exp(-gamma*dt)+exp(-delta1*dt) - 1;
    
    prob_R1[0] = 1 - exp(-omega*dt);  // E_1,t this goes back to S_1,(t+1)
    prob_R1[1] = 1 - exp(-delta1*dt);
    prob_R1[2] = exp(-omega*dt) + exp(-delta1*dt) - 1;
    
    prob_S2[0] = 1-exp(-dt*beta2*kappa*I/N);
    prob_S2[1] = 1-exp(-delta2*dt);
    prob_S2[2] = exp(-delta2*dt) + exp(-dt*beta2*kappa*I/N) - 1;
    
    prob_I2[0] = 1-exp(-dt*gamma);
    prob_I2[1] = 1-exp(-dt*delta2);
    prob_I2[2] = exp(-dt*gamma)+exp(-dt*delta2) - 1;
    
    prob_R2[0] = 1 - exp(-dt*omega);  // E_1,t this goes back to S_1,(t+1)
    prob_R2[1] = 1 - exp(-dt*delta2);
    prob_R2[2] = exp(-dt*omega) + exp(-dt*delta2) - 1;
    
    // For Age Group (3): Die first before transition;
    
    int S3mD, I3mD, R3mD;
    
    S3mD = rbinom(S3, 1-dt*mu); // S3 minus Death: mu is the death rate, so it's 1-mu here
    I3mD = rbinom(I3, 1-dt*mu);
    R3mD = rbinom(R3, 1-dt*mu);
    
    prob_S3[0] = 1-exp(-dt*beta3*kappa*I/N);
    prob_S3[1] = exp(-dt*beta3*kappa*I/N);
    
    prob_I3[0] = 1 - exp(-dt*gamma);
    prob_I3[1] = exp(-dt*gamma);
    
    prob_R3[0] = 1 - exp(-dt*omega);
    prob_R3[1] = exp(-dt*omega);
    
    // Transition
    // B: S->I
    // C: I->R
    // F: Aging: (1)->(2)->(3)
    // E: R->S
    // D: Death
    //// Note: Here S_1, S_2... are all old value from (t-1)
    rmultinom(S1, &prob_S1, 3, &trans_S1); // B, F, S-B-F
    rmultinom(I1, &prob_I1, 3, &trans_I1); // C, F, I-C-F
    rmultinom(R1, &prob_R1, 3, &trans_R1); // E, F, R-E-F
    
    rmultinom(S2, &prob_S2, 3, &trans_S2); // B, F, S-B-F
    rmultinom(I2, &prob_I2, 3, &trans_I2); // C, F, I-C-F
    rmultinom(R2, &prob_R2, 3, &trans_R2); // E, F, R-E-F
    
    rmultinom(S3mD, &prob_S3, 2, &trans_S3); // B, (S-D)-B
    rmultinom(I3mD, &prob_I3, 2, &trans_I3); // C, (I-D)-C
    rmultinom(R3mD, &prob_R3, 2, &trans_R3); // E, (R-D)-E
    
    S1 = trans_S1[2] + trans_R1[0] + rpois(4*1025.7); // Include Birth
    I1 = trans_I1[2] + trans_S1[0];
    R1 = trans_R1[2] + trans_I1[0];
    
    S2 = trans_S2[2] + trans_R2[0] + trans_S1[1]; // Include Aging
    I2 = trans_I2[2] + trans_S2[0] + trans_I1[1];
    R2 = trans_R2[2] + trans_I2[0] + trans_R1[1];
    
    S3 = trans_S3[1] + trans_R3[0] + trans_S2[1]; // Include Aging
    I3 = trans_I3[1] + trans_S3[0] + trans_I2[1];
    R3 = trans_R3[1] + trans_I3[0] + trans_R2[1];
    
    //Accumvar
    H1 += trans_S1[0];
    H2 += trans_S2[0];
    H3 += trans_S3[0];
    
    q1 = -1; 
    while(q1 < 0 || q1 > 1){
      q1 = rnorm(0.07, sigma_q);
    }
    
    q2 = -1; 
    while(q2 < 0 || q2 > 1){
      q2 = rnorm(0.07, sigma_q);
    }
    
    q3 = -1; 
    while(q3 < 0 || q3 > 1){
      q3 = rnorm(0.07, sigma_q);
    }
")


# define parameters (without betas)
params_fixed <- c(gamma=1, delta1=1/(5*52),delta2=1/(55*52), alpha=1/(78.86912*52), 
                  mu=0, N=82372825, omega=1/(1*52))
# full-estimate-rinit
rinit <- Csnippet("
    double m = N/(S10+I10+R10+S20+I20+R20+S30+I30+R30);
    I1=nearbyint(m*I10);
    I2=nearbyint(m*I20);
    I3=nearbyint(m*I30);
    S1=nearbyint(m*S10);
    S2=nearbyint(m*S20);
    S3=nearbyint(m*S30);
    R1=nearbyint(m*R10);
    R2=nearbyint(m*R20);
    R3=nearbyint(m*R30);
    H1 = 0;
    H2 = 0;
    H3 = 0;
")

# WWR's rinit
rinit_wwr <- Csnippet("
    S1=3876549;
    S2=57139612;
    S3=19573727;
    I1=30351;
    I2=871;
    I3=2550;
    R1=1315221;
    R2=302852;
    R3=131092;
    H1 = 0;
    H2 = 0;
    H3 = 0;
")

# x <- c(3876549,57139612,19573727,30351,871,2550,1315221,302852,131092)
# x/sum(x)

# Set to MLE
wwr_mle <- params_fixed

wwr_mle <- c(params_fixed, 
             "beta1" = 11.48,
             "beta2" = 0.25,
             "beta3" = 0.35,
             "phi" = 0.14,
             "beta11" = 0.16,
             "sigma_q" = 0.021,
             "sigma_xi" = 66.89,
             "S10" = 4.706102e-02,
             "S20" = 6.936707e-01,
             "S30" = 2.376236e-01,
             "I10" = 3.684589e-04,
             "I20" = 1.057388e-05,
             "I30" = 3.095681e-05,
             "R10" = 1.596669e-02,
             "R20" = 3.676601e-03,
             "R30" = 1.591447e-03)


pt <- pomp::parameter_trans(
  log = c("beta1","beta2","beta3","sigma_q","sigma_xi"),
  logit=c("beta11"),
  barycentric=c("S10","I10","R10",
                "S20","I20","R20",
                "S30","I30","R30"),
  toEst= pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst= pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)

read.table("real_rotavirus_metadata.txt") %>%
  rbind(data.frame(time=0,cases1=NA,cases2=NA,cases3=NA)) %>%
  arrange(time) -> dat

pomp(data = dat,
     times="time",
     t0=0,
     dmeasure = dmeas,
     rmeasure = rmeas,
     rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
     statenames = c("S1", "I1", "R1", "H1", 
                    "S2", "I2", "R2", "H2",
                    "S3", "I3", "R3", "H3", 
                    "q1", "q2", "q3"),
     paramnames = names(wwr_mle),
     accumvars=c("H1", "H2", "H3"),
     rinit=rinit,
     partrans = pt,
     params = wwr_mle
) -> sir


# 
# sir_panel <- panelPomp::panelPomp(list(unit1=sir),
#                                   shared=NULL,
#                                   specific=wwr_mle |> 
#                                     as.matrix() |>
#                                     `colnames<-`("unit1")
# )
@


% Maximization Round 1

<<maximization1>>=
set.seed(123)
sir_box <- list(
  beta1=c(10,15),
  beta2=c(0.2,0.4),
  beta3=c(0.3,0.5),
  phi=c(0.01,0.3),
  beta11=c(0.1,0.2),
  sigma_q=c(0.001,0.1),
  sigma_xi=c(65,70),
  S10 = c(0.01,0.05),
  I10 = c(0.0001, 0.0005),
  R10 = c(0.01, 0.05),
  S20 = c(0.01,0.7),
  I20 = c(0.000001,0.00002),
  R20 = c(0.001,0.005),
  S30 = c(0.01,0.5),
  I30 = c(0.000001, 0.0001),
  R30 = c(0.0001,0.005),
  #fixed params
  gamma = c(1,1),
  delta1=c(1/(5*52),1/(5*52)),
  delta2=c(1/(55*52),1/(55*52)),
  alpha=c(1/(78.86912*52),1/(78.86912*52)),
  mu=c(0,0),
  N=c(82372825,82372825),
  omega=c(1/(1*52),1/(1*52))
)


design_matrix <- runif_design(lower = sapply(sir_box, `[`, 1),
                              upper = sapply(sir_box, `[`, 2),
                              n = ncores)

# c(apply(sir_box,1,function(x)runif(1,min=x[1],max=x[2])),
#   params_fixed) -> starting


bake(file = paste0(out_dir, "round_01/ovov_mif_01.rds"),{ 
  registerDoFuture()
  plan(multicore, workers=ncores)
  mifs_global_01 <- foreach(i=1:ncores,.options.future=list(seed=652643293)) %dofuture% {
    pomp::mif2(
      sir,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(beta1=0.01,beta2=0.01,beta3=0.01,
                    beta11=0.01,phi=0.01,sigma_q=0.01,sigma_xi=0.01,
                    S10=ivp(0.24),I10=ivp(0.24),R10=ivp(0.24),
                    S20=ivp(0.24),I20=ivp(0.24),R20=ivp(0.24),
                    S30=ivp(0.24),I30=ivp(0.24),R30=ivp(0.24)),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = design_matrix[i, ]
    ) 
  }
  mifs_global_01
})

bake(file = paste0(out_dir, "round_01/ovov_el_01.rds"),{
  el_01 <- eval_logLik_pomp(mifs_global_01, ncores, NP_MIF, 4248930)
  el_01
})
@


% Continue: Maximization Round 2

<<maximization2>>=
set.seed(1234)
sd_02 = 2/3

el_01 <- readRDS(paste0(out_dir,"round_01/ovov_el_01.rds"))
starting_values_02 <- choose_n_fits(el_01, top_n_fits, ncores)


bake(file = paste0(out_dir,"round_02/ovov_mif_02.rds"),{ 
  registerDoFuture()
  plan(multicore, workers=ncores)
  mifs_global_02 <- foreach(i=1:ncores,.options.future=list(seed=652643293)) %dofuture% {
    pomp::mif2(
      sir,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(beta1=0.01*sd_02,beta2=0.01*sd_02,beta3=0.01*sd_02,
                    beta11=0.01*sd_02,phi=0.01*sd_02,
                    sigma_q=0.01*sd_02,sigma_xi=0.01*sd_02,
                    S10=ivp(0.24)*sd_02,I10=ivp(0.24)*sd_02,R10=ivp(0.24)*sd_02,
                    S20=ivp(0.24)*sd_02,I20=ivp(0.24)*sd_02,R20=ivp(0.24)*sd_02,
                    S30=ivp(0.24)*sd_02,I30=ivp(0.24)*sd_02,R30=ivp(0.24)*sd_02),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = starting_values_02[[i]]
    ) 
  }
  mifs_global_02
})

bake(file = paste0(out_dir, "round_02/ovov_el_02.rds"),{
  el_02 <- eval_logLik_pomp(mifs_global_02, ncores, NP_MIF, seed=4248930)
  el_02
})
@

% Continue: Maximization Round 3

<<maximization3>>=
set.seed(12345)
sd_03           = (2/3)^2

el_02 <- readRDS(paste0(out_dir, "round_02/ovov_el_02.rds"))
starting_values_03 <- choose_n_fits(el_02, top_n_fits, ncores)


bake(file = paste0(out_dir, "round_03/ovov_mif_03.rds"),{ 
  registerDoFuture()
  plan(multicore, workers=ncores)
  mifs_global_03 <- foreach(i=1:ncores,.options.future=list(seed=652643293)) %dofuture% {
    pomp::mif2(
      sir,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(beta1=0.01*sd_03,beta2=0.01*sd_03,beta3=0.01*sd_03,
                    beta11=0.01*sd_03,phi=0.01*sd_03,
                    sigma_q=0.01*sd_03,sigma_xi=0.01*sd_03,
                    S10=ivp(0.24)*sd_03,I10=ivp(0.24)*sd_03,R10=ivp(0.24)*sd_03,
                    S20=ivp(0.24)*sd_03,I20=ivp(0.24)*sd_03,R20=ivp(0.24)*sd_03,
                    S30=ivp(0.24)*sd_03,I30=ivp(0.24)*sd_03,R30=ivp(0.24)*sd_03),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = starting_values_03[[i]]
    ) 
  }
  mifs_global_03
})

bake(file = paste0(out_dir,"round_03/ovov_el_03.rds"),{
  el_03 <- eval_logLik_pomp(mifs_global_03, ncores, NP_MIF, seed=4248930)
  el_03
})
@


<<estimated-mle>>=
mif_el <- readRDS(paste0(out_dir,"round_03/ovov_el_03.rds")) 
# Can be changed if more rounds needed
est_mle_vec <- mif_el[which.max(mif_el$logLik),]
@

%%%%%%%%%% 11111111 %%%%%%%%%%%%

\section{Introduction}

This article results from an investigation of the results in Table~1 of \citet{wwr}.
The authors of that paper were given the opportunity to submit a correction, after we shared the results of our investigation with them, but they declined.
Our task here is to correct the error in Table~1 so that researchers considering whether to implement Poisson approximate likelihood (PAL) are appropriately informed about its benefits.
Table~1 reanalyzes the model and data of \citet{stocks}, for which the likelihood was calculated using a particle filter.
The theory developed by \cite{wwr} shows that PAL has some potentially useful scaling properties, but the numerical results in Table~1 appear to show much stronger performance than the particle filter on this example of moderate size.

First, we show that most of this apparent improvement for PAL arises because \cite{wwr} used a different scaling of the data from \cite{stocks}.
%% \section{Comparing PAL with the particle filter} 
Two models for the same data can properly be compared by their likelihood, even if the models have entirely different structures.
One can making allowance for the number of estimated parameters using a quantity such as Akaike's information criterion [REF].
However, if data are rescaled, a correction is required to make likelihoods comparable.
If one model describes a dataset in grams, and another describes it in kilograms, then the latter model will earn an increased log-likelihood of $\log(10^3)$ for each data point simply because of the change in scale.
Presenting a direct comparison of a likelihood for the data in grams with a likelihood for the data in kilograms would evidently be inapprpriate.
\cite{stocks} fitted their model to a dataset derived by dividing the original reported count data by an estimated reporting rate, to put their data on the scale of the actual number of cases in the population.
\cite{wwr} fitted directly to the report data.
The reporting rate used by \cite{stocks} varied over time, but was generally around $7\%$.
On 1200 data points, this corresponds to a discrepancy of around $-1200\log(0.07)=3200$ log-likelihood units, largely explaining the difference reported in Table~1 and interpreted by \cite{wwr} as evidence supporting PAL.
The comparison can be corrected either by applying the method of \cite{stocks} to the data of \cite{wwr} or vvice versa.
Since the method of \cite{stocks} is applicable to a more general class of models, and supported by published software, it was more convenient to apply this method to the model and data of \cite{wwr}.
The large discrepancy in log-likelihood disappears at this point (see our Table~\ref{tab:ovovrealdata} , column \ed{XXX}).
This re-analysis does, however, show a small advantage for PAL.
We continued our investigation to establish the cause of this.

<<pf-sim-100>>=
# Set WWR's MLE
wwr_mle_raw <- c(params_fixed, 
             "beta1" = 11.48,
             "beta2" = 0.25,
             "beta3" = 0.35,
             "phi" = 0.14,
             "beta11" = 0.16,
             "sigma_q" = 0.021,
             "sigma_xi" = 66.89)


i <- 1

bake(file=paste0(out_dir, "pfilter/ovov_sim_100.rds"),{
  ovov_sim_100 <- list()
  registerDoFuture()
  plan(multicore, workers=ncores)
  
  repeat {
    # Simulation
    sim1 <- simulate(t0 = 0,
                     times = c(1:416),
                     dmeasure = dmeas,
                     rmeasure = rmeas,
                     rprocess = discrete_time(rproc, delta.t = 1/4),
                     statenames = c("S1", "I1", "R1", "H1", 
                                    "S2", "I2", "R2", "H2",
                                    "S3", "I3", "R3", "H3",
                                    "q1", "q2", "q3"),
                     obsnames = c("cases1", "cases2", "cases3"),
                     paramnames = names(wwr_mle_raw),
                     accumvars = c("H1", "H2", "H3"),
                     rinit = rinit_wwr,
                     params = wwr_mle_raw)
    
    sim_data <- as.data.frame(sim1)
    if (!any(sim_data == 0)) {
      ovov_sim_100[[length(ovov_sim_100) + 1]] <- sim_data[, c("cases1", "cases2", "cases3")]
      i <- i + 1  # Increment only if the condition is satisfied
    } else {
      print(paste("Condition not met for iteration", i, "- retrying"))
    }
    
    if (length(ovov_sim_100) >= sim_rep) break  # Exit the loop when the desired number of successful iterations is reached
  }
  ovov_sim_100
})

# load("ovov_sim_100.Rdata")
@


<<pfilter_sim_100>>=
ovov_sim_100 <- readRDS(paste0(out_dir, "pfilter/ovov_sim_100.rds"))
bake(file=paste0(out_dir, "pfilter/pfilter_ovov_sim_100.rds"),{
  pfilter_ovov_sim_100 <- c()
  registerDoFuture()
  plan(multicore, workers=ncores)
  
  for(i in 1:length(ovov_sim_100)){
    dat <- data.frame(time=c(1:416), ovov_sim_100[[i]])
    dat |>
      pomp(t0=0,
           time="time",
           rprocess=discrete_time(rproc,delta.t=1/4), ## Obs time every week, but delta.t transition is actually everyday
           rinit=rinit_wwr,
           dmeasure=dmeas,
           rmeasure=rmeas,
           accumvars=c("H1","H2","H3"),
           statenames=c("S1","S2","S3",
                        "I1","I2","I3",
                        "R1","R2","R3",
                        "H1","H2","H3",
                        "q1","q2","q3"),
           paramnames = names(wwr_mle_raw)
      ) -> sir_wwr
    
    theta <- wwr_mle_raw
    
    foreach(i=1:ncores, .combine=c,
            .options.future=list(seed=998468235L)
    ) %dofuture% {
      pfilter(sir_wwr,Np=NP_MIF,save.states=T, params=theta)
    } -> pfs
    
    pfilter_ovov_sim_100[i] <-logmeanexp(logLik(pfs))  
  }
  pfilter_ovov_sim_100
})
@

<<palfilter_sim_100>>=
ovov_sim_100 <- readRDS(paste0(out_dir, "pfilter/ovov_sim_100.rds"))
prop <- 420

init_dist <- c(3876549, 30351, 1315221, 57139612, 871, 302852, 19573727, 2550, 131092)
# initial_guess <- c(runif(1,5,20),runif(1,0.05,0.5),runif(1,0.2,1),runif(1,0.05,0.4),runif(1,0.05,0.4),runif(1,0.05,0.2),runif(1,10,100))
initial_guess <- c(11.48,0.25,0.35,0.14,0.16,0.021,66.89)

### Just a filter
Sys.time()

bake(file=paste0(out_dir, "pfilter/palfilter_ovov_100.rds"),{
  y <- c()
  registerDoFuture()
  plan(multicore, workers=ncores)
  for(i in 1:length(ovov_sim_100)){
    realdat <- ovov_sim_100[[i]] |> as.matrix()
    lik_list <- rotavirus_SMC_qropxi(init_dist,realdat , 9, 
                                     c(initial_guess[1],initial_guess[2],initial_guess[3],
                                       initial_guess[4],initial_guess[5]), 
                                     gamma_par = c(initial_guess[7],  1/initial_guess[7]), 
                                     norm_par = c(0.07,initial_guess[6]), 
                                     t(prop), NP_MIF,1)
    y[i] <- lik_list$log_lik
  }
  y
})

Sys.time()

# lik_list$ll_storage |> sum()

# x <- c()
# for(i in 1:10){
#   x[i] <- any(ovov_sim_100[[i]]==0)
# }
@
       

\begin{figure}[h]
\centering
\includegraphics[width=0.55\textwidth]{thesis-figs/PAL_vs_SMC_ovov_100.png}
\caption{\label{fig:PAL_vs_SMC_ovov_100}Log-likelihoods computed using two filtering methods for 100 randomly simulated datasets at the MLE of the OvOv model.
The particle filter and PAL used 50,000 particles.
The red line is the line $Lik(pfilter)=Lik(palfilter)$.
On average, the particle filter likelihood is $7.7$ log units higher than the PAL.
%%  -6224.245 - (-6231.97) = 7.725
}
\end{figure}


PAL provides an approximate filter, whereas the particle filter provides a Monte~Carlo estimate of an exact filter.
Possible explanations for PAL obtaining a higher log-likelihood than a particle filter include:
\begin{enumerate}
\item The particle filter could have high Monte Carlo variance. This results in negative bias in its log-likilihood estimate due to Jensen's inequality, since the particle filter provides an unbiased likelihood estimate.
\item If the model is misspecified, the approximation error in PAL could lead to a higher log-likelihood estimate than that of the actual misspecified model.
\end{enumerate}
Log-likelihood is a proper scoring rule for forecasts \cite{gneiting2}, and both the particle filter and PAL construct their log-likelihood estimates via a sequence of one-step forecasts.
%% resulting in estimates of the conditional log-likelihood, $f_{Y
Therefore, if the model is correctly specified, the approximation error in PAL can only decrease the expected log-likelihood.
We tested this on simulated data, for which the model of \cite{wwr} is correctly specified.
We found that, in this case, the particle filter out-performs PAL (Figure~\ref{fig:PAL_vs_SMC_ovov_100}).
This supports hypothesis 2, above.
Consequently, we looked for model misspecification by comparing the conditional log-likelihood estimates at each time point, as described in the Methods section.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{thesis-figs/cond_SMC_PAL_realdat.png}
\caption{\label{fig:cond_SMC_PAL_realdat}Conditional log-likelihoods computed using two methods for the real rotavirus dataset, with the SMC method replicated 36 times due to its high variances. The blue line represents overlapping results from 36 SMC computations, while the red line is derived from PAL. The main sources of likelihood shortfall of SMC are at time points $t=1, 2, 3, 11, 81, 194$, and $325-333$.}
\end{figure}

Figure~ \ref{fig:cond_SMC_PAL_realdat} indicates that the worst conditional log-likelihood values, estimated using the particle filter, arise at the start of the time series.
This could be a problem with data collection, or some subtle issue with the science of the epidemiological system, but a simple possibility is that the initial values of the latent state process might be poorly specified.
For this model, the initial values were fixed based on scientific reasoning, rather than being estimated.
If we instead estimate the initial values, we obtain the results in Table~\ref{tab:mlebywwr}. 

\begin{table}[htbp]
  \centering
  \begin{threeparttable}
    \caption{Maximum likelihood estimation of parameters by PAL for three models \citep{wwr}, and by mif2 algorithm \citep{pomppackagepaper} for the OvOv model.
    \ed{LET'S DISCUSS WHETHER WE WANT TO SHOW JUST OV-OV HERE. IT WOULD BE SIMPLER.}
    }
    \label{tab:mlebywwr}
    \begin{tabular}{>{\raggedright\arraybackslash}p{2cm} *{3}{>{\centering\arraybackslash}p{1.5cm}} >{\centering\arraybackslash}p{2.5cm}}
      \toprule
      Parameter      & EqEq  & EqOv  & OvOv  & OvOv (\textbf{pomp})$^*$ \\
      \midrule
      \(\beta_1\)    & 12.15 & 12.74 & 11.48 & $\Sexpr{round(est_mle_vec["beta1"],2)}$\\
      \(\beta_2\)    & 0.22  & 0.21  & 0.25  & $\Sexpr{round(est_mle_vec["beta2"],2)}$\\
      \(\beta_3\)    & 0.34  & 0.31  & 0.35  & $\Sexpr{round(est_mle_vec["beta3"],2)}$\\
      \(\phi\)       & 0.017 & 0.14  & 0.14  & $\Sexpr{round(est_mle_vec["phi"],2)}$\\
      \(\rho\)       & 0.022 & 0.19  & 0.16  & $\Sexpr{round(est_mle_vec["beta11"],2)}$\\
      \(\sigma^2_q\) & n/a   & 0.042 & 0.021 & $\Sexpr{round(est_mle_vec["sigma_q"],3)}$\\
      \(\sigma_\xi\) & n/a   & n/a   & 66.89 & $\Sexpr{round(est_mle_vec["sigma_xi"],2)}$\\
      \addlinespace
      \multicolumn{1}{l}{\(S_{10}\)} & \multicolumn{3}{c}{$3876549^*$} & $\Sexpr{round(pop*est_mle_vec["S10"],0)}$\\
      \multicolumn{1}{l}{\(I_{10}\)} & \multicolumn{3}{c}{$30351$}     & $\Sexpr{round(pop*est_mle_vec["I10"],0)}$\\
      \multicolumn{1}{l}{\(R_{10}\)} & \multicolumn{3}{c}{$1315221$}   & $\Sexpr{round(pop*est_mle_vec["R10"],0)}$\\
      \multicolumn{1}{l}{\(S_{20}\)} & \multicolumn{3}{c}{$57139612$}  & $\Sexpr{round(pop*est_mle_vec["S20"],0)}$\\
      \multicolumn{1}{l}{\(I_{20}\)} & \multicolumn{3}{c}{$871$}       & $\Sexpr{round(pop*est_mle_vec["I20"],0)}$\\
      \multicolumn{1}{l}{\(R_{20}\)} & \multicolumn{3}{c}{$302852$}    & $\Sexpr{round(pop*est_mle_vec["R20"],0)}$\\
      \multicolumn{1}{l}{\(S_{30}\)} & \multicolumn{3}{c}{$19573727$}  & $\Sexpr{round(pop*est_mle_vec["S30"],0)}$\\
      \multicolumn{1}{l}{\(I_{30}\)} & \multicolumn{3}{c}{$2550$}      & $\Sexpr{round(pop*est_mle_vec["I30"],0)}$\\
      \multicolumn{1}{l}{\(R_{30}\)} & \multicolumn{3}{c}{$131092$}    & $\Sexpr{round(pop*est_mle_vec["R30"],0)}$\\
      \midrule
      \addlinespace
      \textbf{AIC}  & $98866.65$ & $15154.75$ & $13778.08$ & $\Sexpr{round((-2)*((length(wwr_mle) - length(params_fixed)) - est_mle_vec["logLik"]),2)}$\\
      \addlinespace
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \small
      \item[*] The initial distribution parameter \(\lambda_0 = (S_{10}, I_{10}, R_{10}, S_{20}, I_{20}, R_{20}, S_{30}, I_{30}, R_{30})\) are assumed to be \textbf{fixed} in \cite{wwr}. The results of the estimates of the OvOv (\textbf{pomp}) model are obtained by using the Iterated Filtering algorithm with 3 rounds and 100 iterations, 50,000 particles, and 36 replicates in each round with the top 12 best fits in terms of likelihood chosen to be the starting value for the next round. 
    \end{tablenotes}
  \end{threeparttable}
\end{table}



\ed{WE CAN REVIEW EXACTLY WHICH NUMBERS TO SHOW. IDEALLY, AS LITTLE AS POSSIBLE IN ORDER TO MAKE A STRONG ARGUMENT.}



We conclude that PAL is a potentially useful algorithm, with some favorable theoretical properties.
However, the corrected evidence does not indicate an advantage for using PAL in situations where the particle filter is effective.

%%%%%%%%% 222222222 %%%%%%%%%%5

\section*{Methods}

\ed{BRIEFLY DESCRIBE HERE ANY DETAILS NEEDED TO DESCRIBE THE RESULTS. COMMENT ON CODE AVAILABILITY.}

To evaluate whether SMC performs as well as PAL in terms of both computational time and likelihood identifiability, we replicated the model using the R package \textbf{pomp} \citep{pomppackagepaper}. First, we verified that the two models are essentially identical. Here, we simulated data from the model 1000 times at the MLE shown in Table \ref{tab:mlebywwr} and compared the summary statistics of 1000 simulated time series from two models using the t-test. Summary statistics including the mean, median, and variance for all three age groups were tested. The results showed no significant differences between the two models from which the data were simulated, confirming that both the latent process model and the observation model are essentially consistent with each other, given that the observations were directly simulated.

From Table \ref{tab:model_performance}, it is evident that the OvOv model, with overdispersion in both the dynamic system and the measurement model, yields the lowest AIC.
We therefore focus on this version of the model.

This article has focused on likelihood evaluation, but the inferences presented also require the likelihood estimate to maximized.
\cite{stocks} used iterated filtering to maximize the particle filter likelihood estimate, whereas \cite{wwr} used direct numerical maximization.
We follow the approaches adopted by these two papers.
The optimization method is not directly pertinent to the comparison of the filtering methods, so we do not discuss optimization in further detail here.

An extended description of our reanalysis of \cite{wwr} is provided by \cite{hao24}.
That thesis also contains some additional results reinforcing the conclusions presented in this article.


\begin{table}[ht] % The placement specifier can be [h!tbp]
\centering % Centers the table
\caption{log-likelihood at the MLE of OvOv model for the real rotavirus data, computed using two filtering methods, employed 36 replicates and 50,000 particles} % This is effectively the table title
\label{tab:ovovrealdata} % For referencing this table elsewhere in your text
\begin{tabular}{|c c c|} 
 \hline
  & PAL & SMC  \\
  \hline
 OvOv & -6892.168 & -7200.866 \\ 
 \hline
\end{tabular}
\end{table}


\bibliography{bib-pal}

\end{document}
