\documentclass[10pt]{article}
\usepackage{color}
\usepackage{graphicx} 
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{makecell}
\usepackage{array} 
\usepackage{subcaption}
\usepackage{setspace} 
\usepackage{float} 
\usepackage{tabularx}
\usepackage[round]{natbib}
\usepackage[font={small,it}]{caption}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{amsfonts,amssymb,amsthm}
\usepackage{url}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}

\bibliographystyle{apalike}

\begin{document}

\title{Poisson approximate likelihood compared to the particle filter}
\author{Yize Hao, Aaron A. Abkemeier and Edward L. Ionides}
\date{Draft compiled on \today}
\maketitle

\begin{abstract}
Filtering algorithms are fundamental for inference on partially observed stochastic dynamic systems, since they provide access to the likelihood function and hence enable likelihood-based or Bayesian inference.
A novel Poisson approximate likelihood (PAL) filter was introduced by \cite{wwr}.
PAL employs a Poisson approximation to conditional densities, offering a fast approximation to the likelihood function for a certain subset of partially observed Markov process models.
A central piece of evidence for PAL is the comparison in Table~1 of \cite{wwr}, which claims a large improvement for PAL over a standard particle filter algorithm.
This evidence, based on a model and data from a previous scientific study by \citet{stocks}, might suggest that researchers confronted with similar models should use PAL rather than particle filter methods.
Taken at face value, this evidence also reduces the credibility of \cite{stocks} by indicating a shortcoming with the numerical methods that they used.
However, we show that the comparison of log-likelihood values made by \cite{wwr} is flawed because their PAL calculations were carried out using a dataset scaled differently from the previous study.
If PAL and the particle filter are applied to the same data, the advantage claimed for PAL disappears.
On simulations where the model is correctly specified, the particle filter outperforms PAL.

\end{abstract}

<<setup,echo=FALSE,message=FALSE,warning=FALSE>>=
rm(list = ls())     # clear objects
library("knitr")
opts_knit$set(concordance=TRUE)
opts_chunk$set(
  tidy = TRUE,
# cache = TRUE,
  cache = FALSE,  # set cache = FALSE when editing the code
# message = FALSE,
# warning = FALSE,
  tidy.opts = list(
    keep.blank.line = FALSE
  ),
  comment = "",
  echo = FALSE,
  results = FALSE,
  dev.args = list(
    bg = "transparent",
    pointsize = 9
  ),
  fig.path = "figure/"
)

myround <- function(x, digits = 1) {
  # taken from the broman package
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

graphics.off()      # close graphics windows
library(pomp)
library(ggplot2)
library(scales)
library(foreach)
library(doParallel)
library(doFuture)
library(dplyr)
library(Rcpp)
library(RcppArmadillo)
sourceCpp('rotavirus_normq.cpp')
source("choose_n_fits.R")
source("eval_logLik_pomp.R")

RUN_LEVEL = 3
NP_MIF       = switch(RUN_LEVEL, 10,   5000,  50000)
NMIF         = switch(RUN_LEVEL, 5,    20,    100)
ncores       = switch(RUN_LEVEL, 4,    10,    36)
top_n_fits   = switch(RUN_LEVEL, 1,    2,     12)
sim_rep      = switch(RUN_LEVEL, 5,    20,    100) #sim_ovov

#population number
pop <- 82372825
time <- c(1:416)

out_dir <- paste0("output_0",RUN_LEVEL, "/")
@


% OvOv model is here

<<OvOv-model, echo=F, warning=F, message=F>>=
# measurement model 
dmeas <- Csnippet("
  if(ISNA(cases1)) {
    lik = (give_log) ? 0 : 1;
  } else {
      lik =  dbinom(cases1, H1, q1, 1) +
      dbinom(cases2, H2, q2, 1) +
      dbinom(cases3, H3, q3, 1);
    }
  lik = (give_log) ? lik : exp(lik);"
)

rmeas <-  Csnippet("
  cases1 = rbinom(H1, q1);
  cases2 = rbinom(H2, q2);
  cases3 = rbinom(H3, q3);
")

rproc <- Csnippet("
  int I = I1 + I2 + I3;
  int trans_S1[3], trans_S2[3], trans_S3[2], trans_I1[3], trans_I2[3], 
  trans_I3[2], trans_R1[3], trans_R2[3], trans_R3[2];
    
  double prob_S1[3], prob_I1[3], prob_R1[3], prob_S2[3], prob_I2[3], prob_R2[3],
  prob_S3[2], prob_I3[2], prob_R3[2];
    
  double xi = rgamma(sigma_xi, 1/sigma_xi);
    
  double kappa = (1 + beta11*cos(2*3.141593*t/52 + phi)) * xi;
    
  // Define rate
  prob_S1[0] = 1 - exp(-dt*beta1*kappa*I/N); // 0->1
  prob_S1[1] = 1 - exp(-delta1*dt);
  prob_S1[2] = exp(-delta1*dt) + exp(-dt*beta1*kappa*I/N) - 1;
    
  prob_I1[0] = 1 - exp(-gamma*dt);
  prob_I1[1] = 1 - exp(-delta1*dt);
  prob_I1[2] = exp(-gamma*dt)+exp(-delta1*dt) - 1;
    
  prob_R1[0] = 1 - exp(-omega*dt);  // E_1,t this goes back to S_1,(t+1)
  prob_R1[1] = 1 - exp(-delta1*dt);
  prob_R1[2] = exp(-omega*dt) + exp(-delta1*dt) - 1;
    
  prob_S2[0] = 1 - exp(-dt*beta2*kappa*I/N);
  prob_S2[1] = 1 - exp(-delta2*dt);
  prob_S2[2] = exp(-delta2*dt) + exp(-dt*beta2*kappa*I/N) - 1;
    
  prob_I2[0] = 1 - exp(-dt*gamma);
  prob_I2[1] = 1 - exp(-dt*delta2);
  prob_I2[2] = exp(-dt*gamma)+exp(-dt*delta2) - 1;
    
  prob_R2[0] = 1 - exp(-dt*omega);  // E_1,t this goes back to S_1,(t+1)
  prob_R2[1] = 1 - exp(-dt*delta2);
  prob_R2[2] = exp(-dt*omega) + exp(-dt*delta2) - 1;
    
  // For Age Group (3): Die first before transition;
    
  int S3mD, I3mD, R3mD;
    
  S3mD = rbinom(S3, 1-dt*mu); // S3 minus Death: mu is the death rate, so it's 1-mu here
  I3mD = rbinom(I3, 1-dt*mu);
  R3mD = rbinom(R3, 1-dt*mu);
    
  prob_S3[0] = 1-exp(-dt*beta3*kappa*I/N);
  prob_S3[1] = exp(-dt*beta3*kappa*I/N);
    
  prob_I3[0] = 1 - exp(-dt*gamma);
  prob_I3[1] = exp(-dt*gamma);
    
  prob_R3[0] = 1 - exp(-dt*omega);
  prob_R3[1] = exp(-dt*omega);
    
  // Transition
  // B: S->I
  // C: I->R
  // F: Aging: (1)->(2)->(3)
  // E: R->S
  // D: Death
  //// Note: Here S_1, S_2... are all old values from (t-1)
  rmultinom(S1, &prob_S1, 3, &trans_S1); // B, F, S-B-F
  rmultinom(I1, &prob_I1, 3, &trans_I1); // C, F, I-C-F
  rmultinom(R1, &prob_R1, 3, &trans_R1); // E, F, R-E-F
    
  rmultinom(S2, &prob_S2, 3, &trans_S2); // B, F, S-B-F
  rmultinom(I2, &prob_I2, 3, &trans_I2); // C, F, I-C-F
  rmultinom(R2, &prob_R2, 3, &trans_R2); // E, F, R-E-F
    
  rmultinom(S3mD, &prob_S3, 2, &trans_S3); // B, (S-D)-B
  rmultinom(I3mD, &prob_I3, 2, &trans_I3); // C, (I-D)-C
  rmultinom(R3mD, &prob_R3, 2, &trans_R3); // E, (R-D)-E
    
  S1 = trans_S1[2] + trans_R1[0] + rpois(4*1025.7); // Include Birth
  I1 = trans_I1[2] + trans_S1[0];
  R1 = trans_R1[2] + trans_I1[0];
    
  S2 = trans_S2[2] + trans_R2[0] + trans_S1[1]; // Include Aging
  I2 = trans_I2[2] + trans_S2[0] + trans_I1[1];
  R2 = trans_R2[2] + trans_I2[0] + trans_R1[1];
    
  S3 = trans_S3[1] + trans_R3[0] + trans_S2[1]; // Include Aging
  I3 = trans_I3[1] + trans_S3[0] + trans_I2[1];
  R3 = trans_R3[1] + trans_I3[0] + trans_R2[1];
    
  //Accumvar
  H1 += trans_S1[0];
  H2 += trans_S2[0];
  H3 += trans_S3[0];
    
  q1 = -1; 
  while(q1 < 0 || q1 > 1){
    q1 = rnorm(0.07, sigma_q);
  }
    
  q2 = -1; 
  while(q2 < 0 || q2 > 1){
    q2 = rnorm(0.07, sigma_q);
  }
    
  q3 = -1; 
  while(q3 < 0 || q3 > 1){
    q3 = rnorm(0.07, sigma_q);
  }
")


# full-estimate-rinit
rinit_full <- Csnippet("
  double m = N/(S10+I10+R10+S20+I20+R20+S30+I30+R30);
  I1=nearbyint(m*I10);
  I2=nearbyint(m*I20);
  I3=nearbyint(m*I30);
  S1=nearbyint(m*S10);
  S2=nearbyint(m*S20);
  S3=nearbyint(m*S30);
  R1=nearbyint(m*R10);
  R2=nearbyint(m*R20);
  R3=nearbyint(m*R30);
  H1 = 0;
  H2 = 0;
  H3 = 0;
")

# WWR's rinit
rinit_wwr <- Csnippet("
  S1=3876549;
  S2=57139612;
  S3=19573727;
  I1=30351;
  I2=871;
  I3=2550;
  R1=1315221;
  R2=302852;
  R3=131092;
  H1 = 0;
  H2 = 0;
  H3 = 0;
")

rinit_stocks <- Csnippet("
  S1=5.09484e+06;
  S2=5.73856e+07;
  S3=1.96976e+07;
  I1=2871;
  I2=639;
  I3=174;
  R1=124410;
  R2=57072;
  R3=9578;
  H1 = 0;
  H2 = 0;
  H3 = 0;
")

# define parameters (fixed)
params_fixed <- c(
  gamma = 1, 
  delta1 = 1/(5*52),
  delta2 = 1/(55*52), 
  alpha = 1/(78.86912*52), 
  mu = 0, 
  N = 82372825, 
  omega = 1/(1*52)
)
# Set to MLE
params_wwr <- c(
  params_fixed, 
  "beta1" = 11.48,
  "beta2" = 0.25,
  "beta3" = 0.35,
  "phi" = 0.14,
  "beta11" = 0.16,
  "sigma_q" = 0.021,
  "sigma_xi" = 66.89
)

params_full <- c(
  params_wwr,
  "S10" = 4.706102e-02,
  "S20" = 6.936707e-01,
  "S30" = 2.376236e-01,
  "I10" = 3.684589e-04,
  "I20" = 1.057388e-05,
  "I30" = 3.095681e-05,
  "R10" = 1.596669e-02,
  "R20" = 3.676601e-03,
  "R30" = 1.591447e-03
)

pt_full <- pomp::parameter_trans(
  log = c("beta1", "beta2", "beta3", "sigma_q", "sigma_xi"),
  logit = c("beta11"),
  barycentric = c("S10","I10","R10",
                  "S20","I20","R20",
                  "S30","I30","R30"),
  toEst = pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst = pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)

pt_wwr <- pomp::parameter_trans(
  log = c("beta1", "beta2", "beta3", "sigma_q", "sigma_xi"),
  logit = c("beta11"),
  toEst = pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst = pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)


dat <- read.table("unscaled_rotavirus_data.txt") |>
  arrange(time)

statenames <- c(
  "S1", "I1", "R1", "H1",
  "S2", "I2", "R2", "H2",
  "S3", "I3", "R3", "H3",
  "q1", "q2", "q3"
)

sir_wwr <- pomp(
  data = dat,
  times = "time",
  t0 = 0,
  dmeasure = dmeas,
  rmeasure = rmeas,
  rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
  statenames = statenames,
  paramnames = names(params_wwr),
  accumvars = c("H1", "H2", "H3"),
  rinit = rinit_wwr,
  partrans = pt_wwr,
  params = params_wwr
) 

sir_full <- pomp(
  sir_wwr,
  rinit = rinit_full,
  params = params_full,
  partrans = pt_full,
  statenames = statenames,
  paramnames = names(params_full)
)
@


% Maximization Round 1 - Full rinit: also estimate initial values

<<maximization1, echo=F, warning=F, message=F>>=
set.seed(123)
sir_box <- list(
  beta1 = c(10, 15),
  beta2 = c(0.2, 0.4),
  beta3 = c(0.3, 0.5),
  phi = c(0.01, 0.3),
  beta11 = c(0.1, 0.2),
  sigma_q = c(0.001, 0.1),
  sigma_xi = c(65, 70),
  S10 = c(0.01, 0.05),
  I10 = c(0.0001, 0.0005),
  R10 = c(0.01, 0.05),
  S20 = c(0.01, 0.7),
  I20 = c(0.000001, 0.00002),
  R20 = c(0.001, 0.005),
  S30 = c(0.01, 0.5),
  I30 = c(0.000001, 0.0001),
  R30 = c(0.0001, 0.005),
  gamma = c(1, 1),
  delta1 = c(1/(5*52), 1/(5*52)),
  delta2 = c(1/(55*52), 1/(55*52)),
  alpha = c(1/(78.86912*52), 1/(78.86912*52)),
  mu = c(0, 0),
  N = c(82372825, 82372825),
  omega = c(1/(1*52), 1/(1*52))
)

design_matrix <- runif_design(
  lower = sapply(sir_box, `[`, 1),
  upper = sapply(sir_box, `[`, 2),
  n = ncores
)

bake(file = paste0(out_dir, "mif2_maximization/full/round_01/mifs_global_01.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      sir_full,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(
        beta1 = 0.01, beta2 = 0.01, beta3 = 0.01, beta11 = 0.01, 
        phi = 0.01, sigma_q = 0.01, sigma_xi = 0.01,
        S10 = ivp(0.24), I10 = ivp(0.24), R10 = ivp(0.24),
        S20 = ivp(0.24), I20 = ivp(0.24), R20 = ivp(0.24),
        S30 = ivp(0.24), I30 = ivp(0.24), R30 = ivp(0.24)
      ),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = design_matrix[i, ]
    ) 
  }
}) -> mifs_global_01 


bake(file = paste0(out_dir, "mif2_maximization/full/round_01/el_01.rds"),{
  eval_logLik_pomp(mifs_global_01, ncores, NP_MIF, 4248930)
}) ->  el_01

@

% Round 2 - Full rinit

<<maximization2, echo=F, warning=F, message=F>>=
set.seed(1234)
sd_02 = 2/3
starting_values_02 <- choose_n_fits(el_01, top_n_fits, ncores)

bake(file = paste0(out_dir,"mif2_maximization/full/round_02/mifs_global_02.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_01[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_02, beta2 = 0.01*sd_02, beta3 = 0.01*sd_02,
        beta11 = 0.01*sd_02, phi = 0.01*sd_02,
        sigma_q = 0.01*sd_02, sigma_xi = 0.01*sd_02,
        S10 = ivp(0.24)*sd_02, I10 = ivp(0.24)*sd_02, R10 = ivp(0.24)*sd_02,
        S20 = ivp(0.24)*sd_02, I20 = ivp(0.24)*sd_02, R20 = ivp(0.24)*sd_02,
        S30 = ivp(0.24)*sd_02, I30 = ivp(0.24)*sd_02, R30 = ivp(0.24)*sd_02
      ),
      params = starting_values_02[[i]]
    ) 
  }
}) ->  mifs_global_02


bake(file = paste0(out_dir, "mif2_maximization/full/round_02/el_02.rds"),{
  eval_logLik_pomp(mifs_global_02, ncores, NP_MIF, seed=4248930)
}) ->  el_02
@

% Round 3 - Full rinit

<<maximization3, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_03 = (2/3)^2
starting_values_03 <- choose_n_fits(el_02, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/full/round_03/mifs_global_03.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_02[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_03, beta2 = 0.01*sd_03, beta3 = 0.01*sd_03,
        beta11 = 0.01*sd_03, phi = 0.01*sd_03,
        sigma_q = 0.01*sd_03, sigma_xi = 0.01*sd_03,
        S10 = ivp(0.24)*sd_03, I10 = ivp(0.24)*sd_03, R10 = ivp(0.24)*sd_03,
        S20 = ivp(0.24)*sd_03, I20 = ivp(0.24)*sd_03, R20 = ivp(0.24)*sd_03,
        S30 = ivp(0.24)*sd_03, I30 = ivp(0.24)*sd_03, R30 = ivp(0.24)*sd_03
      ),
      params = starting_values_03[[i]]
    ) 
  }
}) -> mifs_global_03


bake(file = paste0(out_dir,"mif2_maximization/full/round_03/el_03.rds"),{
  eval_logLik_pomp(mifs_global_03, ncores, NP_MIF, seed = 4248930)
}) ->  el_03
@

<<estimated-mle, echo=FALSE, message=FALSE, warning=FALSE>>=
# Can be changed if more rounds needed
est_mle_vec <- el_03[which.max(el_03$logLik),]
@


% Maximization Round 1 - Fix initial values with Stocks' rinit

<<maximization1-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(123)

dat %>%
  rbind(data.frame(time=0,cases1=NA,cases2=NA,cases3=NA)) %>%
  arrange(time) -> dat_sbh

sir_sbh_init <- pomp(
  data = dat_sbh,
  times = "time",
  t0 = 1-6*52,
  dmeasure = dmeas,
  rmeasure = rmeas,
  rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
  statenames = statenames,
  paramnames = names(params_wwr),
  accumvars = c("H1", "H2", "H3"),
  rinit = rinit_stocks,
  partrans = pt_wwr,
  params = params_wwr
)

sir_box_sbh <- list(
  beta1 = c(10, 15),
  beta2 = c(0.2, 0.4),
  beta3 = c(0.3, 0.5),
  phi = c(0.01, 0.3),
  beta11 = c(0.1, 0.2),
  sigma_q = c(0.001, 0.1),
  sigma_xi = c(65, 70),
  gamma = c(1, 1),
  delta1 = c(1/(5*52), 1/(5*52)),
  delta2 = c(1/(55*52), 1/(55*52)),
  alpha = c(1/(78.86912*52), 1/(78.86912*52)),
  mu = c(0, 0),
  N = c(82372825, 82372825),
  omega = c(1/(1*52), 1/(1*52))
)

design_matrix_sbh <- runif_design(
  lower = sapply(sir_box_sbh, `[`, 1),
  upper = sapply(sir_box_sbh, `[`, 2),
  n = ncores
)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_01/mifs_global_sbh_01.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      sir_sbh_init,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(
        beta1 = 0.01, beta2 = 0.01, beta3 = 0.01, beta11 = 0.01, 
        phi = 0.01, sigma_q = 0.01, sigma_xi = 0.01
      ),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = design_matrix_sbh[i, ]
    ) 
  }
}) -> mifs_global_sbh_01 


bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_01/sbh_el_01.rds"),{
  eval_logLik_pomp(mifs_global_sbh_01, ncores, NP_MIF, 4248930)
}) -> sbh_el_01
@

% Round 2 - Stocks rinit

<<maximization2-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(1234)
sd_02 = 2/3
starting_values_sbh_02 <- choose_n_fits(sbh_el_01, top_n_fits, ncores)

bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_02/mifs_global_sbh_02.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_01[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_02, beta2 = 0.01*sd_02, beta3 = 0.01*sd_02,
        beta11 = 0.01*sd_02, phi = 0.01*sd_02,
        sigma_q = 0.01*sd_02, sigma_xi = 0.01*sd_02
      ),
      params = starting_values_sbh_02[[i]]
    ) 
  }
}) ->  mifs_global_sbh_02


bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_02/sbh_el_02.rds"),{
  eval_logLik_pomp(mifs_global_sbh_02, ncores, NP_MIF, seed=4248930)
}) -> sbh_el_02
@

% Round 3 - Stocks rinit

<<maximization3-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_03 = (2/3)^2
starting_values_sbh_03 <- choose_n_fits(sbh_el_02, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_03/mifs_global_sbh_03.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_02[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_03, beta2 = 0.01*sd_03, beta3 = 0.01*sd_03,
        beta11 = 0.01*sd_03, phi = 0.01*sd_03,
        sigma_q = 0.01*sd_03, sigma_xi = 0.01*sd_03
      ),
      params = starting_values_sbh_03[[i]]
    ) 
  }
}) -> mifs_global_sbh_03


bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_03/sbh_el_03.rds"),{
  eval_logLik_pomp(mifs_global_sbh_03, ncores, NP_MIF, seed = 4248930)
}) ->  sbh_el_03
@

% Round 4 - Stocks rinit

<<maximization4-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_04 = (2/3)^3
starting_values_sbh_04 <- choose_n_fits(sbh_el_03, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_04/mifs_global_sbh_04.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_03[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_04, beta2 = 0.01*sd_04, beta3 = 0.01*sd_04,
        beta11 = 0.01*sd_04, phi = 0.01*sd_04,
        sigma_q = 0.01*sd_04, sigma_xi = 0.01*sd_04
      ),
      params = starting_values_sbh_04[[i]]
    ) 
  }
}) -> mifs_global_sbh_04


bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_04/sbh_el_04.rds"),{
  eval_logLik_pomp(mifs_global_sbh_04, ncores, NP_MIF, seed = 4248930)
}) ->  sbh_el_04
@

% Round 5 - Stocks rinit

<<maximization5-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_05 = (2/3)^4
starting_values_sbh_05 <- choose_n_fits(sbh_el_04, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_05/mifs_global_sbh_05.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_04[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_05, beta2 = 0.01*sd_05, beta3 = 0.01*sd_05,
        beta11 = 0.01*sd_05, phi = 0.01*sd_05,
        sigma_q = 0.01*sd_05, sigma_xi = 0.01*sd_05
      ),
      params = starting_values_sbh_05[[i]]
    ) 
  }
}) -> mifs_global_sbh_05


bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_05/sbh_el_05.rds"),{
  eval_logLik_pomp(mifs_global_sbh_05, ncores, NP_MIF, seed = 4248930)
}) ->  sbh_el_05
@

% Round 6 - Stocks rinit

<<maximization6-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_06 = (2/3)^5
starting_values_sbh_06 <- choose_n_fits(sbh_el_05, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_06/mifs_global_sbh_06.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_05[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_06, beta2 = 0.01*sd_06, beta3 = 0.01*sd_06,
        beta11 = 0.01*sd_06, phi = 0.01*sd_06,
        sigma_q = 0.01*sd_06, sigma_xi = 0.01*sd_06
      ),
      params = starting_values_sbh_06[[i]]
    ) 
  }
}) -> mifs_global_sbh_06


bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_06/sbh_el_06.rds"),{
  eval_logLik_pomp(mifs_global_sbh_06, ncores, NP_MIF, seed = 4248930)
}) ->  sbh_el_06
@

<<estimated-mle_stocks_init, echo=FALSE, message=FALSE, warning=FALSE>>=
# Can be changed if more rounds needed
est_mle_vec_sbh <- sbh_el_06[which.max(sbh_el_06$logLik),]
@


%%%%%%%%%% 11111111 %%%%%%%%%%%%

% \section{Introduction}

This article results from an investigation of the results presented by \citet{wwr} (henceforth, WWR) in their Table~1.
WWR were given the opportunity to submit a correction, after we shared the results of our investigation with them, but they declined.
The theory developed by WWR shows that their Poisson Approximate Likelihood (PAL) method has some potentially useful scaling properties.
This theory is supported by numerical results, in their Table~1, which erroneously claim to show that PAL has substantially stronger performance than a particle filter (PF) on an example of scientific interest.
We present corrected results so that researchers considering whether to implement PAL are appropriately informed about its benefits.

Table~1 of WWR uses a model and data adapted from \citet{stocks} (henceforth, SBH).
SBH used PF to calculate the likelihood for a stochastic dynamic model of rotavirus transmission.
SBH found clear evidence for the importance of including overdispersion in the model for their epidemiological data.
This is significant because most earlier research on population dynamics avoided consideration of overdispersion, perhaps due to the lack of available statistical methodology for fitting overdispersed nonlinear stochastic dynamic models. 
The conclusions of SBH hinge on a comparison of likelihoods, and so the results of WWR discredit those conclusions by indicating that SBH based their reasoning on inaccurately computed likelihoods.
An important consequence of correcting Table~1 of WWR is that the results of SBH stand undiminished. 

SBH and WWR each fitted three different rotavirus models.
The first has equidispersion (i.e., no overdispersion) in the measurement model and the dynamic model, and is called EqEq by WWR.
The second, EqOv, includes overdispersion in only the measurement model.
The third, OvOv, includes overdispersion in both these model components.
We focus on OvOv, which WWR and SBH both found to be the best fitting model. 

We show that the claimed advantage for PAL over PF, on the OvOv model, arose because WWR used a different scaling of the data from SBH.
Two models for the same data can properly be compared by their likelihood, even if the models have entirely different structures.
Allowance for the number of estimated parameters can be made using a quantity such as Akaike's information criterion \citep{Akaike1974ANL}.
However, if data are rescaled, an adjustment is required to make likelihoods comparable.
For example, if one model describes a dataset in grams and another describes it in kilograms, then the latter model will earn an increased log-likelihood of $\log(10^3)$ for each data point simply because of the change in scale.
Presenting a direct comparison of a likelihood for the data in grams with a likelihood for the data in kilograms would evidently be inappropriate.


<<sim-100, echo=F, warning=F, message=F>>=
bake(file = paste0(out_dir, "pfilter/simulated_dataset/ovov_sim_100.rds"),{
  ovov_sim_computation <- list()
  i <- 1
  repeat {
    # Simulation
    sim_data <- simulate(sir_wwr, format = "data.frame") |>
      subset(select=c("time", "cases1", "cases2", "cases3"))
    
    if (!any(sim_data == 0)) {
      ovov_sim_computation[[length(ovov_sim_computation) + 1]] <- sim_data
      i <- i + 1  
    } else {
      print(paste("Condition not met for iteration", i, "- retrying"))
    }
    
    if (length(ovov_sim_computation) >= sim_rep) break  
  }
  ovov_sim_computation
}) -> ovov_sim_100
@


<<pfilter_sim_100, echo=FALSE, message=FALSE, warning=FALSE, results='hide'>>=
bake(file = paste0(out_dir, "pfilter/simulated_dataset/pfilter_ovov_sim_100.rds"),{
  pfilter_obj <- list()
  pfilter_ovov_sim_computation <- c()
  
  for(i in 1:length(ovov_sim_100)){
    pomp(
      data = ovov_sim_100[[i]],
      times = "time",
      t0 = 0,
      dmeasure = dmeas,
      rmeasure = rmeas,
      rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
      statenames = statenames,
      paramnames = names(params_wwr),
      accumvars = c("H1", "H2", "H3"),
      rinit = rinit_wwr,
      partrans = pt_wwr,
      params = params_wwr
    )  -> pfilter_obj[[i]]
    
    plan(multicore, workers = ncores)
    
    foreach(j = 1:ncores, .combine = c,
      .options.future = list(seed = 998468235L)
    ) %dofuture% {
      pfilter(
        pfilter_obj[[i]], Np = NP_MIF, save.states = FALSE, params = params_wwr
      )
    } -> pfs
    pfilter_ovov_sim_computation[i] <- logmeanexp(logLik(pfs))  
  }
  pfilter_ovov_sim_computation
}) ->  pfilter_ovov_sim_100
@



<<palfilter_sim_100, echo=FALSE, message=FALSE, warning=FALSE, results='hide'>>=
prop <- 420
init_dist <- c(
  3876549, 30351, 1315221, 57139612, 871, 302852, 19573727, 2550, 131092
)
initial_guess <- c(11.48, 0.25, 0.35, 0.14, 0.16, 0.021, 66.89)

bake(file = paste0(out_dir, "pfilter/simulated_dataset/palfilter_ovov_sim_100.rds"),{
  plan(multicore, workers = ncores)
  foreach(
    i = 1:length(ovov_sim_100), .combine = c, .options.future = list(seed = 998468235L)
  ) %dofuture% {
    pal_sim_dat <- ovov_sim_100[[i]][,-1] |> as.matrix()
    lik_list <- rotavirus_SMC_qropxi(
      init_dist = init_dist, 
      y_obs = pal_sim_dat, 
      m = 9, 
      regular_params = c(
        initial_guess[1], initial_guess[2], initial_guess[3], initial_guess[4],
        initial_guess[5]
      ), 
      gamma_par = c(initial_guess[7], 1/initial_guess[7]), 
      norm_par = c(0.07, initial_guess[6]), 
      prop = t(prop), 
      n_particles = NP_MIF,
      ncores = ncores
    )
    lik_list$log_lik
  }
}) -> palfilter_ovov_sim_100

# lik_list$ll_storage |> sum()
@

<<pfilter_wwr, echo=FALSE, message=FALSE, warning=FALSE>>=
bake(file = paste0(out_dir, "pfilter/original_dataset/pfilter_wwr.rds"),{
  plan(multicore, workers = ncores)
  foreach(
    i = 1:ncores, .combine = c, .options.future = list(seed = 998468235L)
  ) %dofuture% {
      pfilter(sir_wwr, Np = NP_MIF, params = params_wwr)
    } 
}) ->  pfilter_wwr

bake(file = paste0(out_dir, "pfilter/original_dataset/pfilter_wwr_cond_lik.rds"),{
  pfilter_wwr_cond_lik_computation <- list()
  for(k in 1:ncores){
    pfilter_wwr_cond_lik_computation[[k]] <- pfilter_wwr[[k]]@cond.logLik
  }
  pfilter_wwr_cond_lik_computation
}) -> pfilter_wwr_cond_lik
@

<<palfilter_wwr, echo=FALSE>>=
realdat <- as.matrix(dat[,-1])

bake(file = paste0(out_dir,"pfilter/original_dataset/palfilter_lik_list_wwr.rds"),{
 set.seed(1223)
 rotavirus_SMC_qropxi(
    init_dist = init_dist, 
    y_obs = realdat, 
    m = 9, 
    regular_params = c(
      initial_guess[1], initial_guess[2], initial_guess[3], initial_guess[4],
      initial_guess[5]
    ), 
    gamma_par = c(initial_guess[7], 1/initial_guess[7]), 
    norm_par = c(0.07, initial_guess[6]), 
    prop = t(prop), 
    n_particles = NP_MIF, 
    ncores = ncores
  ) 
}) -> palfilter_lik_list_wwr
@

<<simulate-initial-for-palfilter>>=
n_sim <- 1000

sim_sbh_init_for_pal <- simulate(
  sir_sbh_init,
  nsim=n_sim,
  params=est_mle_vec_sbh,
  seed = 123456
)

s1_t1_vec <- c()
i1_t1_vec <- c()
r1_t1_vec <- c()
s2_t1_vec <- c()
i2_t1_vec <- c()
r2_t1_vec <- c()
s3_t1_vec <- c()
i3_t1_vec <- c()
r3_t1_vec <- c()

for(i in 1:n_sim){
  s1_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["S1",2]
  i1_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["I1",2]
  r1_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["R1",2]
  
  s2_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["S2",2]
  i2_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["I2",2]
  r2_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["R2",2]
  
  s3_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["S3",2]
  i3_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["I3",2]
  r3_t1_vec[i] <- sim_sbh_init_for_pal[[i]]@states["R3",2]
}

sbh_mean_state <- c(
  S1=round(mean(s1_t1_vec),0),
  I1=round(mean(i1_t1_vec),0),
  R1=round(mean(r1_t1_vec),0),
  S2=round(mean(s2_t1_vec),0),
  I2=round(mean(i2_t1_vec),0),
  R2=round(mean(r2_t1_vec),0),
  S3=round(mean(s3_t1_vec),0),
  I3=round(mean(i3_t1_vec),0),
  R3=round(mean(r3_t1_vec),0)
)

#calculate the difference in the init dist
diff <- sum(sbh_mean_state) - pop
#reduce the differenc proportionally
init_dist_sbh_mle <- round(sbh_mean_state - diff*(sbh_mean_state/sum(sbh_mean_state)),0)
#check if init_dist == pop
sum(init_dist_sbh_mle) == pop

#sum(init_dist_sbh_mle) == pop
@

<<palfilter-at-mle_sbh, echo=FALSE, message=FALSE, warning=FALSE>>=
prop <- 420

initial_guess_mif2_mle <- c(
  as.numeric(est_mle_vec_sbh["beta1"]), 
  as.numeric(est_mle_vec_sbh["beta2"]), 
  as.numeric(est_mle_vec_sbh["beta3"]),
  as.numeric(est_mle_vec_sbh["phi"]),
  as.numeric(est_mle_vec_sbh["beta11"]),
  as.numeric(est_mle_vec_sbh["sigma_q"]),
  as.numeric(est_mle_vec_sbh["sigma_xi"])
)

bake(file = paste0(out_dir,"pfilter/original_dataset/palfilter_mif2_mle.rds"),{
 set.seed(1223)
 rotavirus_SMC_qropxi(
    init_dist = init_dist_sbh_mle, 
    y_obs = realdat, 
    m = 9, 
    regular_params = c(
      initial_guess_mif2_mle[1], 
      initial_guess_mif2_mle[2], 
      initial_guess_mif2_mle[3], 
      initial_guess_mif2_mle[4],
      initial_guess_mif2_mle[5]
    ), 
    gamma_par = c(initial_guess_mif2_mle[7], 1/initial_guess_mif2_mle[7]), 
    norm_par = c(0.07, initial_guess_mif2_mle[6]), 
    prop = t(prop), 
    n_particles = NP_MIF, 
    ncores = ncores
  ) 
}) -> palfilter_mif2_mle
@

<<benchmark,echo=FALSE>>=
set.seed(123)

sbh_data <- read.csv("sbh_data.txt",sep=" ",skip=3)[,-c(1)]
sbh_benchmark <- sum(apply(sbh_data,2, function(x)
  arima(log(x + 1), order =c(2,0,1))$aic + 2 * sum(log(x+1))
))

wwr_benchmark <- sum(apply(realdat,2, function(x)
  arima(log(x + 1), order =c(2,0,1))$aic + 2 * sum(log(x+1))
))

tab_signif <- 0
@

% \section{Results}

\begin{table}[ht] % The placement specifier can be [h!tbp]
\centering 
\caption{AIC for the OvOv rotavirus model, computed using two filtering methods. PAL is the Poisson approximate likelihood, implemented using the code of WWR. PF is the particle filter, implemented using the R package pomp \citep{pomppackagepaper}. Lines 1, 2 and 7 are taken from WWR, and the remainder are our own computations. Line 3 recomputes the previously published value in Line 2, and the small difference is presumably due to rounding in Table~2 of WWR. We used $5 \times 10^4$ particles for both PF and PAL. PF was repeated 36 times to reduce the Monte Carlo variance, but this step was not necessary for PAL due to its lower Monte~Carlo variance. PF results were maximized using iterated filtering, following the approach of SBH. PAL results were maximized using coordinate gradient descent, using the code of WWR.
} 
\label{tab:ovovrealdata}
\begin{tabular}{lllllr} 
  \hline
  & Method & Data & Model & Parameters                          & AIC  \\
  \hline
  1. & PF  & Rescaled counts  & SBH   & Table~2 of SBH &  \Sexpr{myround(20134.38,tab_signif)}
  \\
  2. & PAL & Counts  & WWR   & Table~2 of WWR                      & \Sexpr{myround(13778.08,tab_signif)}
  \\
  3. & PAL & Counts  & WWR   & Table~2 of WWR  &
    \Sexpr{myround(-2*palfilter_lik_list_wwr$log_lik + 2*7 ,tab_signif)} 
  \\
  4. & PF  & Counts & WWR    & Table~2 of WWR &
    \Sexpr{myround(-2*logmeanexp(logLik(pfilter_wwr)) + 2*7,tab_signif)}
  \\
  5. & PF  & Counts & WWR modified   & Maximum likelihood &
     \Sexpr{myround(-2*est_mle_vec_sbh["logLik"] + 2*7,tab_signif)}
  \\
  6. & PAL & Counts & WWR modified   & Matching line 5 &
     \Sexpr{myround(-2*palfilter_mif2_mle$log_lik + 2*7 ,tab_signif)}
  \\   
  7. & Benchmark & Rescaled counts & log-ARMA(2,1)    &  & 23043
  \\
  8. & Benchmark & Counts & log-ARMA(2,1)      &  &
    \Sexpr{myround(wwr_benchmark,tab_signif)} 
  \\
 \hline
\end{tabular}
\end{table}


SBH fitted their model to a dataset of rescaled counts derived by dividing the original reported count data by an estimated reporting rate.
Thus, SBH fitted to data on the scale of the disease incidence in the population.
By contrast, WWR fitted directly to the reported case count data.
The reporting rate used by SBH varied over time and location, but was generally around $7\%$.
On $3\times 416$ data points, this corresponds to a discrepancy of $-1248 \, \log(0.07) \approx 3300$ log-likelihood units, largely explaining the difference interpreted by WWR as evidence supporting PAL (Table~\ref{tab:ovovrealdata}, comparing lines 1 and 2).
The comparison between PAL and PF can be corrected by applying the method of SBH to the model and data of WWR, or vice versa.
Since the method of SBH is applicable to a more general class of models, and supported by widely-used software, it was convenient to apply the SBH method to the model and data of WWR.
The large discrepancy in log-likelihood disappears when recomputing the likelihood using PF for the model fitted via PAL (Table~\ref{tab:ovovrealdata}, comparing lines 3 and 4).
Some discrepancy remains, and we continued our investigation to establish the cause of this.

Inspection of log-likelihood anomalies \citep{wheeler24,li24} showed that the initial conditions for the latent process in January 2001 were fixed at values which were incompatible with the trajectory of the data early in the time series \citep[Figure~3 of][]{hao24}.
By contrast, SBH fixed their initial conditions 6 years before the first measurement, giving time for the system to reach its equilibrium distribution.
Line 5 of Table~\ref{tab:ovovrealdata} incorporates the SBH specification of initial values into the model of WWR.
The likelihood for this modified model was then maximized using an iterated filtering procedure similar to SBH.
Comparison of lines 3 and 5 shows that this improvement enables PF to reach the AIC values attained by PAL, and show a small improvement.
Further, at these maximum likelihood parameter values, we found that PF beats PAL (comparing lines 5 and 6; for line 6 we initialized PAL at the average value of the stochastic initialization used in line 5).

WWR compared their fitted models with a log-ARMA(2,1) benchmark AIC value of 23043 (line 7).
From this high AIC value, they inferred that all the mechanistic models possessing overdispersion have better statistical fit than a simple log-linear time series model.
However, this AIC value corresponds to SBH's data, not the data fitted by WWR.
Thus, line 7 can be properly compared only to line 1 but not to any other line of  Table~\ref{tab:ovovrealdata}. 
We refitted a log-ARMA(2,1) model to the SBH data and obtained a similar AIC value (\Sexpr{myround(sbh_benchmark,tab_signif)}).
Carrying out the same computation for the data fitted by WWR gives a log-ARMA benchmark AIC value of \Sexpr{myround(wwr_benchmark,tab_signif)} (line 8).
Thus, the statistical fit of the mechanistic model considered by WWR is inferior to a simple log-ARMA model.
This holds for all the variants in lines 2--6 of  Table~\ref{tab:ovovrealdata}, regardless of whether PF or PAL is used.
The goal of mechanistic modeling is not necessarily to beat a simple statistical benchmark, but falling far below a simple statistical benchmark is an indication that additional model development could be worthwhile \citep{wheeler24}.
A correct interpretation of Table~\ref{tab:ovovrealdata} is therefore very different to the conclusions drawn by WWR, who compared line 2 inappropriately to lines 1 and 7. 


<<plot1-100-sim-compare,echo=FALSE, fig.height=4, fig.width=5.5, out.width="4.5in",fig.align="center",fig.cap='Log-likelihood computed using PAL and PF for 100 randomly simulated datasets using the model and parameter values of WWR. Simulations with one or more zero counts were disqualified since they resulted in errors for the PAL implementation of WWR. We used $5 \\times 10^4$ particles for both PF and PAL. PF was repeated 36 times to reduce the Monte Carlo variance, but this step was not necessary for PAL due to its lower Monte~Carlo variance. The red line corresponds to equality of the two estimates.'>>=
data <- data.frame(
  pfilter = pfilter_ovov_sim_100, 
  palfilter = palfilter_ovov_sim_100
)

ggplot(data, aes(x = pfilter, y = palfilter)) +
geom_point(shape = 20, size = 0.3) +  # pch=20, cex=0.3 equivalent
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(
  title=NULL,
  x = "PF",
  y = "PAL"
) +
theme_minimal()
@

In the presence of model misspecification, it becomes difficult to compare likelihood evaluation methods.
A likelihood approximation, such as PAL, may potentially obtain a higher value than the exact likelihood if it compensates for model misspecification.
Log-likelihood is a proper scoring rule for forecasts \citep{gneiting2}, and both the particle filter and PAL construct their log-likelihood estimates via a sequence of one-step forecasts.
Therefore, if the model is correctly specified, the approximation error in PAL can only decrease the expected log-likelihood.
We tested this on simulated data for which the model of WWR is correctly specified.
For this simulation study, the particle filter out-performs PAL (Figure~\ref{fig:plot1-100-sim-compare}).
On average, the particle filter likelihood estimate is $\Sexpr{myround(mean(pfilter_ovov_sim_100)-mean(palfilter_ovov_sim_100),1)}$ log units higher than the PAL.
We know from the benchmark AIC value in Table~\ref{tab:ovovrealdata} (comparing line 8 to lines 2--6) that there is substantial model misspecification.

% \section{Discussion}

It is currently unclear why the model of SBH beats the log-ARMA benchmark for their data, whereas the model of WWR fits more poorly than the log-ARMA model for its corresponding data.
Additional rounds of model development are required to resolve this, for which it is desirable to employ statistical methods that are broadly applicable both in theory and practice.
Particle filter methods meet this criterion since they have the plug-and-play property \citep{breto09,he10} which is not possessed by PAL.
Although WWR have shown that PAL is a potentially useful algorithm with some favorable theoretical properties, the corrected evidence does not indicate an advantage for using PAL in situations where the particle filter is effective.

The source code for this article is available at \url{https://github.com/ionides/pal-vs-pf} and archived at Zenodo.
An extended description of our methods, together with additional numerical results, is provided by \cite{hao24}.

\bibliography{bib-pal}

\clearpage

\end{document}

