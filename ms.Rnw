\documentclass[10pt]{article}
\usepackage{color}
\usepackage{graphicx} % Required for inserting 
%%\usepackage{algpseudocode} % Include the package for algorithmic environment
\usepackage{algorithm}
\usepackage{booktabs} % For better-quality horizontal lines
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{threeparttable} % For table notes
\usepackage{array} % For table formatting

\usepackage{setspace}
\usepackage{float} % add this in your preamble
\usepackage{amsmath}
\usepackage{tabularx}

\newcommand{\ed}[1]{\textcolor{red}{[EI:#1]}}
\newcommand{\yize}[1]{\textcolor{green}{[YH:#1]}}
\newcommand{\aaron}[1]{\textcolor{blue}{[AA:#1]}}

\usepackage[round]{natbib}

\usepackage{multirow}
\usepackage{fourier} 
\usepackage{booktabs}
\usepackage{array}
\usepackage{makecell}
\usepackage[font={small,it}]{caption}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{tabstackengine}
\usepackage[toc,page]{appendix}
\usepackage{graphicx, fullpage, verbatim, amsmath}
\usepackage{url, amsfonts, amssymb, amsthm,color, enumerate}
\usepackage{placeins, listings, textcomp, mathtools, multicol, tikz}
\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage{bm}



\TABstackMath

\doublespacing
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{graphicx}
\bibliographystyle{apalike}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\newcommand\choleraDeath{\delta_{C}}

% Using \doublespacing in the preamble 
% changes the text to double-line spacing
\doublespacing


\begin{document}

\title{Poisson approximate likelihood compared to the particle filter}
\author{Yize Hao, Aaron A. Abkemeier and Edward L. Ionides}
\date{Draft compiled on \today}
\maketitle

\begin{abstract}
Filtering algorithms are fundamental for inference on partially observed stochastic dynamic systems, since they provide access to the likelihood function and hence enable likelihood-based or Bayesian inference.
Recently, a novel Poisson approximate likelihood (PAL) filter was introduced by \cite{wwr}.
PAL employs a Poisson approximation to conditional densities, offering a fast and consistent approximation to the likelihood function for compatible POMP models.
\cite{wwr} supported the importance of their contribution by theoretical analysis and numerical results.
A central piece of evidence for PAL is the comparison in Table~1 of \cite{wwr}, which claims a large improvement for PAL over a standard particle filter algorithm.
This evidence, based on a model and data from a previous scientific study, suggests at face value that researchers confronted with similar models would be irresponsible not to adopt PAL.
We show that the evidence is flawed because the PAL calculations were carried out using a dataset differently scaled from the previous study.
If PAL and the particle filter are used on this new scale, the superficial advantage of PAL largely disappears.
On simulations where the model is correctly specified, the particle filter outperforms PAL.
If a poorly fitting specification of initial value parameters is ammended, the particle filter also outperforms PAL on the actual data.
\end{abstract}


% R starts here

<<setup,echo=FALSE,message=FALSE,warning=FALSE>>=
rm(list = ls())     # clear objects

library("knitr")
opts_knit$set(concordance=TRUE)
opts_chunk$set(
    concordance = TRUE,
    tidy = TRUE,
    message = FALSE,
    warning = FALSE,
    tidy.opts = list(
        keep.blank.line = FALSE
    ),
    comment = "",
    echo = FALSE,
    results = FALSE,
    dev.args = list(
        bg = "transparent",
        pointsize = 9
    ),
    fig.path = "figure/"
)

myround <- function(x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

graphics.off()      # close graphics windows
library(pomp)
library(magrittr)
library(plyr)
library(reshape2)
library(ggplot2)
library(scales)
library(foreach)
library(doParallel)
registerDoParallel()
stopifnot(packageVersion("pomp") >= "1.7")

RUN_LEVEL = 1
NP_MIF       = switch(RUN_LEVEL, 4, 50000)
NMIF         = switch(RUN_LEVEL, 4,  100)
ncores       = switch(RUN_LEVEL, 4,  36)
@


% OvOv model is here

<<OvOv-model>>=
# measurement model 
dmeas <- Csnippet("
                  if (ISNA(cases1)) {
                  lik = (give_log) ? 0 : 1;
                  } else {
                        lik =  dbinom(cases1, H1, q1, 1) +
                        dbinom(cases2, H2, q2, 1) +
                        dbinom(cases3, H3, q3, 1);
                      
                    lik = (give_log) ? lik : exp(lik);
                        
                    }")
rmeas <-  Csnippet("
                    cases1 = rbinom(H1, q1);
                    cases2 = rbinom(H2, q2);
                    cases3 = rbinom(H3, q3);
                  ")



rproc <- Csnippet("
    int I = I1 + I2 + I3;
    int trans_S1[3], trans_S2[3], trans_S3[2], trans_I1[3], trans_I2[3], trans_I3[2], trans_R1[3], trans_R2[3], trans_R3[2];
    
    double prob_S1[3],prob_I1[3],prob_R1[3],prob_S2[3],prob_I2[3],prob_R2[3],prob_S3[2],prob_I3[2],prob_R3[2];
    
    double xi = rgamma(sigma_xi, 1/sigma_xi);
    
    double kappa = (1 + beta11*cos(2*3.141593*t/52 + phi)) * xi;
    
    // Define rate
    prob_S1[0] = 1-exp(-dt*beta1*kappa*I/N); // 0->1
    prob_S1[1] = 1-exp(-delta1*dt);
    prob_S1[2] = exp(-delta1*dt) + exp(-dt*beta1*kappa*I/N) - 1;
    
    prob_I1[0] = 1-exp(-gamma*dt);
    prob_I1[1] = 1-exp(-delta1*dt);
    prob_I1[2] = exp(-gamma*dt)+exp(-delta1*dt) - 1;
    
    prob_R1[0] = 1 - exp(-omega*dt);  // E_1,t this goes back to S_1,(t+1)
    prob_R1[1] = 1 - exp(-delta1*dt);
    prob_R1[2] = exp(-omega*dt) + exp(-delta1*dt) - 1;
    
    prob_S2[0] = 1-exp(-dt*beta2*kappa*I/N);
    prob_S2[1] = 1-exp(-delta2*dt);
    prob_S2[2] = exp(-delta2*dt) + exp(-dt*beta2*kappa*I/N) - 1;
    
    prob_I2[0] = 1-exp(-dt*gamma);
    prob_I2[1] = 1-exp(-dt*delta2);
    prob_I2[2] = exp(-dt*gamma)+exp(-dt*delta2) - 1;
    
    prob_R2[0] = 1 - exp(-dt*omega);  // E_1,t this goes back to S_1,(t+1)
    prob_R2[1] = 1 - exp(-dt*delta2);
    prob_R2[2] = exp(-dt*omega) + exp(-dt*delta2) - 1;
    
    // For Age Group (3): Die first before transition;
    
    int S3mD, I3mD, R3mD;
    
    S3mD = rbinom(S3, 1-dt*mu); // S3 minus Death: mu is the death rate, so it's 1-mu here
    I3mD = rbinom(I3, 1-dt*mu);
    R3mD = rbinom(R3, 1-dt*mu);
    
    prob_S3[0] = 1-exp(-dt*beta3*kappa*I/N);
    prob_S3[1] = exp(-dt*beta3*kappa*I/N);
    
    prob_I3[0] = 1 - exp(-dt*gamma);
    prob_I3[1] = exp(-dt*gamma);
    
    prob_R3[0] = 1 - exp(-dt*omega);
    prob_R3[1] = exp(-dt*omega);
    
    // Transition
    // B: S->I
    // C: I->R
    // F: Aging: (1)->(2)->(3)
    // E: R->S
    // D: Death
    //// Note: Here S_1, S_2... are all old value from (t-1)
    rmultinom(S1, &prob_S1, 3, &trans_S1); // B, F, S-B-F
    rmultinom(I1, &prob_I1, 3, &trans_I1); // C, F, I-C-F
    rmultinom(R1, &prob_R1, 3, &trans_R1); // E, F, R-E-F
    
    rmultinom(S2, &prob_S2, 3, &trans_S2); // B, F, S-B-F
    rmultinom(I2, &prob_I2, 3, &trans_I2); // C, F, I-C-F
    rmultinom(R2, &prob_R2, 3, &trans_R2); // E, F, R-E-F
    
    rmultinom(S3mD, &prob_S3, 2, &trans_S3); // B, (S-D)-B
    rmultinom(I3mD, &prob_I3, 2, &trans_I3); // C, (I-D)-C
    rmultinom(R3mD, &prob_R3, 2, &trans_R3); // E, (R-D)-E
    
    S1 = trans_S1[2] + trans_R1[0] + rpois(4*1025.7); // Include Birth
    I1 = trans_I1[2] + trans_S1[0];
    R1 = trans_R1[2] + trans_I1[0];
    
    S2 = trans_S2[2] + trans_R2[0] + trans_S1[1]; // Include Aging
    I2 = trans_I2[2] + trans_S2[0] + trans_I1[1];
    R2 = trans_R2[2] + trans_I2[0] + trans_R1[1];
    
    S3 = trans_S3[1] + trans_R3[0] + trans_S2[1]; // Include Aging
    I3 = trans_I3[1] + trans_S3[0] + trans_I2[1];
    R3 = trans_R3[1] + trans_I3[0] + trans_R2[1];
    
    //Accumvar
    H1 += trans_S1[0];
    H2 += trans_S2[0];
    H3 += trans_S3[0];
    
    q1 = -1; 
    while(q1 < 0 || q1 > 1){
      q1 = rnorm(0.07, sigma_q);
    }
    
    q2 = -1; 
    while(q2 < 0 || q2 > 1){
      q2 = rnorm(0.07, sigma_q);
    }
    
    q3 = -1; 
    while(q3 < 0 || q3 > 1){
      q3 = rnorm(0.07, sigma_q);
    }
")


# define parameters (without betas)
params_fixed <- c(gamma=1, delta1=1/(5*52),delta2=1/(55*52), alpha=1/(78.86912*52), 
                  mu=0, N=82372825, omega=1/(1*52))
# WWR's rinit
rinit <- Csnippet("
    double m = N/(S10+I10+R10+S20+I20+R20+S30+I30+R30);
    I1=nearbyint(m*I10);
    I2=nearbyint(m*I20);
    I3=nearbyint(m*I30);
    S1=nearbyint(m*S10);
    S2=nearbyint(m*S20);
    S3=nearbyint(m*S30);
    R1=nearbyint(m*R10);
    R2=nearbyint(m*R20);
    R3=nearbyint(m*R30);
    H1 = 0;
    H2 = 0;
    H3 = 0;
")

# Set to MLE
params_stocks_stst_mle <- params_fixed

params_stocks_stst_mle["beta1"] <- 11.48
params_stocks_stst_mle["beta2"] <- 0.25
params_stocks_stst_mle["beta3"] <- 0.35
params_stocks_stst_mle["phi"] <- 0.14
params_stocks_stst_mle["beta11"] <- 0.16
params_stocks_stst_mle["sigma_q"] <- 0.021
params_stocks_stst_mle["sigma_xi"] <- 66.89
params_stocks_stst_mle["S10"] <- 0.047061
params_stocks_stst_mle["I10"] <- 0.000368
params_stocks_stst_mle["R10"] <- 0.015967
params_stocks_stst_mle["S20"] <- 0.015967
params_stocks_stst_mle["I20"] <- 0.000011
params_stocks_stst_mle["R20"] <- 0.003677
params_stocks_stst_mle["S30"] <- 0.237624
params_stocks_stst_mle["I30"] <- 0.000031
params_stocks_stst_mle["R30"] <- 0.001591
@


% Maximization Round 1

<<maximization1>>=
pt <- pomp::parameter_trans(
  log = c("beta1","beta2","beta3","sigma_q","sigma_xi"),
  logit=c("beta11"),
  barycentric=c("S10","I10","R10",
                "S20","I20","R20",
                "S30","I30","R30"),
  toEst= pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst= pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)

read.table("real_rotavirus_metadata.txt") %>%
  rbind(data.frame(time=0,cases1=NA,cases2=NA,cases3=NA)) %>%
  arrange(time) -> dat


pomp(data = dat,
     times="time",
     t0=0,
     dmeasure = dmeas,
     rmeasure = rmeas,
     rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
     statenames = c("S1", "I1", "R1", "H1", 
                    "S2", "I2", "R2", "H2",
                    "S3", "I3", "R3", "H3", 
                    "q1", "q2", "q3"),
     paramnames = names(params_stocks_stst_mle),
     accumvars=c("H1", "H2", "H3"),
     rinit=rinit,
     partrans = pt,
     params = params_stocks_stst_mle
) -> sir

sir_panel <- panelPomp::panelPomp(list(unit1=sir),
                                  shared=NULL,
                                  specific=params_stocks_stst_mle |> 
                                    as.matrix() |>
                                    `colnames<-`("unit1")
)

require(doParallel)
cores <- ncores
registerDoParallel(cores)
mcopts <- list(set.seed=TRUE)


sir_box <- rbind(
  beta1=c(10,15),
  beta2=c(0.2,0.4),
  beta3=c(0.3,0.5),
  phi=c(0.01,0.3),
  beta11=c(0.1,0.2),
  sigma_q=c(0.001,0.1),
  sigma_xi=c(65,70),
  S10 = c(0.01,0.05),
  I10 = c(0.0001, 0.0005),
  R10 = c(0.01, 0.05),
  S20 = c(0.01,0.7),
  I20 = c(0.000001,0.00002),
  R20 = c(0.001,0.005),
  S30 = c(0.01,0.5),
  I30 = c(0.000001, 0.0001),
  R30 = c(0.0001,0.005)
)


## Make it panelPomp
c(apply(sir_box,1,function(x)runif(1,min=x[1],max=x[2])),
  params_fixed) |> 
  as.matrix() |>
  `colnames<-`("unit1") -> starting

bake(file = "output/round_01/ovov_mif_01.rds",{ 
  mifs_global <- foreach(i=1:cores,.packages='pomp', .options.multicore=mcopts) %dopar% {
    panelPomp::mif2(
      sir_panel,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(beta1=0.01,beta2=0.01,beta3=0.01,
                    beta11=0.01,phi=0.01,sigma_q=0.01,sigma_xi=0.01,
                    S10=ivp(0.24),I10=ivp(0.24),R10=ivp(0.24),
                    S20=ivp(0.24),I20=ivp(0.24),R20=ivp(0.24),
                    S30=ivp(0.24),I30=ivp(0.24),R30=ivp(0.24)),
      cooling.type = "geometric",
      Nmif = NMIF,
      shared.start = numeric(0),
      specific.start = starting,
      block = F
    ) 
  }
  mifs_global
})

bake(file = "output/round_01/ovov_01_el.rds",{
  el <- measlespkg::eval_logLik(mifs_global, ncores=cores, np_pf = NP_MIF, nreps=cores)
  el
})
@


% Continue: Maximization Round 2

<<maximization2>>=
sd           = 2/3
top_n_fits   = 12
# measurement model 
dmeas <- Csnippet("
                  if (ISNA(cases1)) {
                  lik = (give_log) ? 0 : 1;
                  } else {
                        lik =  dbinom(cases1, H1, q1, 1) +
                        dbinom(cases2, H2, q2, 1) +
                        dbinom(cases3, H3, q3, 1);
                      
                    lik = (give_log) ? lik : exp(lik);
                        
                    }")
rmeas <-  Csnippet("
                    cases1 = rbinom(H1, q1);
                    cases2 = rbinom(H2, q2);
                    cases3 = rbinom(H3, q3);
                  ")



rproc <- Csnippet("
    int I = I1 + I2 + I3;
    int trans_S1[3], trans_S2[3], trans_S3[2], trans_I1[3], trans_I2[3], trans_I3[2], trans_R1[3], trans_R2[3], trans_R3[2];
    
    double prob_S1[3],prob_I1[3],prob_R1[3],prob_S2[3],prob_I2[3],prob_R2[3],prob_S3[2],prob_I3[2],prob_R3[2];
    
    double xi = rgamma(sigma_xi, 1/sigma_xi);
    
    double kappa = (1 + beta11*cos(2*3.141593*t/52 + phi)) * xi;
    
    // Define rate
    prob_S1[0] = 1-exp(-dt*beta1*kappa*I/N); // 0->1
    prob_S1[1] = 1-exp(-delta1*dt);
    prob_S1[2] = exp(-delta1*dt) + exp(-dt*beta1*kappa*I/N) - 1;
    
    prob_I1[0] = 1-exp(-gamma*dt);
    prob_I1[1] = 1-exp(-delta1*dt);
    prob_I1[2] = exp(-gamma*dt)+exp(-delta1*dt) - 1;
    
    prob_R1[0] = 1 - exp(-omega*dt);  // E_1,t this goes back to S_1,(t+1)
    prob_R1[1] = 1 - exp(-delta1*dt);
    prob_R1[2] = exp(-omega*dt) + exp(-delta1*dt) - 1;
    
    prob_S2[0] = 1-exp(-dt*beta2*kappa*I/N);
    prob_S2[1] = 1-exp(-delta2*dt);
    prob_S2[2] = exp(-delta2*dt) + exp(-dt*beta2*kappa*I/N) - 1;
    
    prob_I2[0] = 1-exp(-dt*gamma);
    prob_I2[1] = 1-exp(-dt*delta2);
    prob_I2[2] = exp(-dt*gamma)+exp(-dt*delta2) - 1;
    
    prob_R2[0] = 1 - exp(-dt*omega);  // E_1,t this goes back to S_1,(t+1)
    prob_R2[1] = 1 - exp(-dt*delta2);
    prob_R2[2] = exp(-dt*omega) + exp(-dt*delta2) - 1;
    
    // For Age Group (3): Die first before transition;
    
    int S3mD, I3mD, R3mD;
    
    S3mD = rbinom(S3, 1-dt*mu); // S3 minus Death: mu is the death rate, so it's 1-mu here
    I3mD = rbinom(I3, 1-dt*mu);
    R3mD = rbinom(R3, 1-dt*mu);
    
    prob_S3[0] = 1-exp(-dt*beta3*kappa*I/N);
    prob_S3[1] = exp(-dt*beta3*kappa*I/N);
    
    prob_I3[0] = 1 - exp(-dt*gamma);
    prob_I3[1] = exp(-dt*gamma);
    
    prob_R3[0] = 1 - exp(-dt*omega);
    prob_R3[1] = exp(-dt*omega);
    
    // Transition
    // B: S->I
    // C: I->R
    // F: Aging: (1)->(2)->(3)
    // E: R->S
    // D: Death
    //// Note: Here S_1, S_2... are all old value from (t-1)
    rmultinom(S1, &prob_S1, 3, &trans_S1); // B, F, S-B-F
    rmultinom(I1, &prob_I1, 3, &trans_I1); // C, F, I-C-F
    rmultinom(R1, &prob_R1, 3, &trans_R1); // E, F, R-E-F
    
    rmultinom(S2, &prob_S2, 3, &trans_S2); // B, F, S-B-F
    rmultinom(I2, &prob_I2, 3, &trans_I2); // C, F, I-C-F
    rmultinom(R2, &prob_R2, 3, &trans_R2); // E, F, R-E-F
    
    rmultinom(S3mD, &prob_S3, 2, &trans_S3); // B, (S-D)-B
    rmultinom(I3mD, &prob_I3, 2, &trans_I3); // C, (I-D)-C
    rmultinom(R3mD, &prob_R3, 2, &trans_R3); // E, (R-D)-E
    
    S1 = trans_S1[2] + trans_R1[0] + rpois(4*1025.7); // Include Birth
    I1 = trans_I1[2] + trans_S1[0];
    R1 = trans_R1[2] + trans_I1[0];
    
    S2 = trans_S2[2] + trans_R2[0] + trans_S1[1]; // Include Aging
    I2 = trans_I2[2] + trans_S2[0] + trans_I1[1];
    R2 = trans_R2[2] + trans_I2[0] + trans_R1[1];
    
    S3 = trans_S3[1] + trans_R3[0] + trans_S2[1]; // Include Aging
    I3 = trans_I3[1] + trans_S3[0] + trans_I2[1];
    R3 = trans_R3[1] + trans_I3[0] + trans_R2[1];
    
    //Accumvar
    H1 += trans_S1[0];
    H2 += trans_S2[0];
    H3 += trans_S3[0];
    
    q1 = -1; 
    while(q1 < 0 || q1 > 1){
      q1 = rnorm(0.07, sigma_q);
    }
    
    q2 = -1; 
    while(q2 < 0 || q2 > 1){
      q2 = rnorm(0.07, sigma_q);
    }
    
    q3 = -1; 
    while(q3 < 0 || q3 > 1){
      q3 = rnorm(0.07, sigma_q);
    }
")


# define parameters (without betas)
params_fixed <- c(gamma=1, delta1=1/(5*52),delta2=1/(55*52), alpha=1/(78.86912*52), 
                  mu=0, N=82372825, omega=1/(1*52))
# WWR's rinit
rinit <- Csnippet("
    double m = N/(S10+I10+R10+S20+I20+R20+S30+I30+R30);
    I1=nearbyint(m*I10);
    I2=nearbyint(m*I20);
    I3=nearbyint(m*I30);
    S1=nearbyint(m*S10);
    S2=nearbyint(m*S20);
    S3=nearbyint(m*S30);
    R1=nearbyint(m*R10);
    R2=nearbyint(m*R20);
    R3=nearbyint(m*R30);
    H1 = 0;
    H2 = 0;
    H3 = 0;
")

# Set to MLE
params_stocks_stst_mle <- params_fixed

params_stocks_stst_mle["beta1"] <- 11.48
params_stocks_stst_mle["beta2"] <- 0.25
params_stocks_stst_mle["beta3"] <- 0.35
params_stocks_stst_mle["phi"] <- 0.14
params_stocks_stst_mle["beta11"] <- 0.16
params_stocks_stst_mle["sigma_q"] <- 0.021
params_stocks_stst_mle["sigma_xi"] <- 66.89
params_stocks_stst_mle["S10"] <- 0.047061
params_stocks_stst_mle["I10"] <- 0.000368
params_stocks_stst_mle["R10"] <- 0.015967
params_stocks_stst_mle["S20"] <- 0.015967
params_stocks_stst_mle["I20"] <- 0.000011
params_stocks_stst_mle["R20"] <- 0.003677
params_stocks_stst_mle["S30"] <- 0.237624
params_stocks_stst_mle["I30"] <- 0.000031
params_stocks_stst_mle["R30"] <- 0.001591



pt <- pomp::parameter_trans(
  log = c("beta1","beta2","beta3","sigma_q","sigma_xi"),
  logit=c("beta11"),
  barycentric=c("S10","I10","R10",
                "S20","I20","R20",
                "S30","I30","R30"),
  toEst= pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst= pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)

read.table("real_rotavirus_metadata.txt") %>%
  rbind(data.frame(time=0,cases1=NA,cases2=NA,cases3=NA)) %>%
  arrange(time) -> dat


pomp(data = dat,
     times="time",
     t0=0,
     dmeasure = dmeas,
     rmeasure = rmeas,
     rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
     statenames = c("S1", "I1", "R1", "H1", 
                    "S2", "I2", "R2", "H2",
                    "S3", "I3", "R3", "H3", 
                    "q1", "q2", "q3"),
     paramnames = names(params_stocks_stst_mle),
     accumvars=c("H1", "H2", "H3"),
     rinit=rinit,
     partrans = pt,
     params = params_stocks_stst_mle
) -> sir

sir_panel <- panelPomp::panelPomp(list(unit1=sir),
                                  shared=NULL,
                                  specific=params_stocks_stst_mle |> 
                                    as.matrix() |>
                                    `colnames<-`("unit1")
)

require(doParallel)
cores <- ncores
registerDoParallel(cores)
mcopts <- list(set.seed=TRUE)

### Next-round code
el <- readRDS("output/round_01/ovov_01_el.rds")
x <- na.omit(el$fits)
score_total = x$logLik
ranking_total = order(score_total, decreasing = TRUE)[1:top_n_fits]

best_fits = dplyr::select(
  x[ranking_total,], -"logLik", -"se"
)

recycle_vec = sort(rep_len(1:top_n_fits, cores))
full_best_fit <- best_fits[recycle_vec, ] 

coef_names <- colnames(full_best_fit)
colnames(full_best_fit) <- gsub(".{7}$","",coef_names)

starting_values <- vector(cores, mode="list")

for(i in 1:cores){
  t(full_best_fit[i, ])  |> 
    as.matrix() |>
    `colnames<-`("unit1") -> starting_values[[i]] 
}

mifs_global <- foreach(i=1:cores,.packages='pomp', .options.multicore=mcopts) %dopar% {
  panelPomp::mif2(
    sir_panel,
    Np = NP_MIF,
    cooling.fraction.50 = 0.5,
    rw.sd = rw_sd(beta1=0.01*sd,beta2=0.01*sd,beta3=0.01*sd,
                  beta11=0.01*sd,phi=0.01*sd,sigma_q=0.01*sd,sigma_xi=0.01*sd,
                  S10=ivp(0.24)*sd,I10=ivp(0.24)*sd,R10=ivp(0.24)*sd,
                  S20=ivp(0.24)*sd,I20=ivp(0.24)*sd,R20=ivp(0.24)*sd,
                  S30=ivp(0.24)*sd,I30=ivp(0.24)*sd,R30=ivp(0.24)*sd),
    cooling.type = "geometric",
    Nmif = NMIF,
    shared.start = numeric(0),
    specific.start = starting_values[[i]],
    block = F
  ) 
}

bake(file = "output/round_02/ovov_mif_02.rds",{ 
  mifs_global
})

bake(file = "output/round_02/ovov_02_el.rds",{
  el <- measlespkg::eval_logLik(mifs_global, ncores = cores, np_pf = NP_MIF, nreps=cores)
  el
})
@

% Continue: Maximization Round 3

<<maximization3>>=
sd           = (2/3)^2
top_n_fits   = 12
# measurement model 
dmeas <- Csnippet("
                  if (ISNA(cases1)) {
                  lik = (give_log) ? 0 : 1;
                  } else {
                        lik =  dbinom(cases1, H1, q1, 1) +
                        dbinom(cases2, H2, q2, 1) +
                        dbinom(cases3, H3, q3, 1);
                      
                    lik = (give_log) ? lik : exp(lik);
                        
                    }")
rmeas <-  Csnippet("
                    cases1 = rbinom(H1, q1);
                    cases2 = rbinom(H2, q2);
                    cases3 = rbinom(H3, q3);
                  ")



rproc <- Csnippet("
    int I = I1 + I2 + I3;
    int trans_S1[3], trans_S2[3], trans_S3[2], trans_I1[3], trans_I2[3], trans_I3[2], trans_R1[3], trans_R2[3], trans_R3[2];
    
    double prob_S1[3],prob_I1[3],prob_R1[3],prob_S2[3],prob_I2[3],prob_R2[3],prob_S3[2],prob_I3[2],prob_R3[2];
    
    double xi = rgamma(sigma_xi, 1/sigma_xi);
    
    double kappa = (1 + beta11*cos(2*3.141593*t/52 + phi)) * xi;
    
    // Define rate
    prob_S1[0] = 1-exp(-dt*beta1*kappa*I/N); // 0->1
    prob_S1[1] = 1-exp(-delta1*dt);
    prob_S1[2] = exp(-delta1*dt) + exp(-dt*beta1*kappa*I/N) - 1;
    
    prob_I1[0] = 1-exp(-gamma*dt);
    prob_I1[1] = 1-exp(-delta1*dt);
    prob_I1[2] = exp(-gamma*dt)+exp(-delta1*dt) - 1;
    
    prob_R1[0] = 1 - exp(-omega*dt);  // E_1,t this goes back to S_1,(t+1)
    prob_R1[1] = 1 - exp(-delta1*dt);
    prob_R1[2] = exp(-omega*dt) + exp(-delta1*dt) - 1;
    
    prob_S2[0] = 1-exp(-dt*beta2*kappa*I/N);
    prob_S2[1] = 1-exp(-delta2*dt);
    prob_S2[2] = exp(-delta2*dt) + exp(-dt*beta2*kappa*I/N) - 1;
    
    prob_I2[0] = 1-exp(-dt*gamma);
    prob_I2[1] = 1-exp(-dt*delta2);
    prob_I2[2] = exp(-dt*gamma)+exp(-dt*delta2) - 1;
    
    prob_R2[0] = 1 - exp(-dt*omega);  // E_1,t this goes back to S_1,(t+1)
    prob_R2[1] = 1 - exp(-dt*delta2);
    prob_R2[2] = exp(-dt*omega) + exp(-dt*delta2) - 1;
    
    // For Age Group (3): Die first before transition;
    
    int S3mD, I3mD, R3mD;
    
    S3mD = rbinom(S3, 1-dt*mu); // S3 minus Death: mu is the death rate, so it's 1-mu here
    I3mD = rbinom(I3, 1-dt*mu);
    R3mD = rbinom(R3, 1-dt*mu);
    
    prob_S3[0] = 1-exp(-dt*beta3*kappa*I/N);
    prob_S3[1] = exp(-dt*beta3*kappa*I/N);
    
    prob_I3[0] = 1 - exp(-dt*gamma);
    prob_I3[1] = exp(-dt*gamma);
    
    prob_R3[0] = 1 - exp(-dt*omega);
    prob_R3[1] = exp(-dt*omega);
    
    // Transition
    // B: S->I
    // C: I->R
    // F: Aging: (1)->(2)->(3)
    // E: R->S
    // D: Death
    //// Note: Here S_1, S_2... are all old value from (t-1)
    rmultinom(S1, &prob_S1, 3, &trans_S1); // B, F, S-B-F
    rmultinom(I1, &prob_I1, 3, &trans_I1); // C, F, I-C-F
    rmultinom(R1, &prob_R1, 3, &trans_R1); // E, F, R-E-F
    
    rmultinom(S2, &prob_S2, 3, &trans_S2); // B, F, S-B-F
    rmultinom(I2, &prob_I2, 3, &trans_I2); // C, F, I-C-F
    rmultinom(R2, &prob_R2, 3, &trans_R2); // E, F, R-E-F
    
    rmultinom(S3mD, &prob_S3, 2, &trans_S3); // B, (S-D)-B
    rmultinom(I3mD, &prob_I3, 2, &trans_I3); // C, (I-D)-C
    rmultinom(R3mD, &prob_R3, 2, &trans_R3); // E, (R-D)-E
    
    S1 = trans_S1[2] + trans_R1[0] + rpois(4*1025.7); // Include Birth
    I1 = trans_I1[2] + trans_S1[0];
    R1 = trans_R1[2] + trans_I1[0];
    
    S2 = trans_S2[2] + trans_R2[0] + trans_S1[1]; // Include Aging
    I2 = trans_I2[2] + trans_S2[0] + trans_I1[1];
    R2 = trans_R2[2] + trans_I2[0] + trans_R1[1];
    
    S3 = trans_S3[1] + trans_R3[0] + trans_S2[1]; // Include Aging
    I3 = trans_I3[1] + trans_S3[0] + trans_I2[1];
    R3 = trans_R3[1] + trans_I3[0] + trans_R2[1];
    
    //Accumvar
    H1 += trans_S1[0];
    H2 += trans_S2[0];
    H3 += trans_S3[0];
    
    q1 = -1; 
    while(q1 < 0 || q1 > 1){
      q1 = rnorm(0.07, sigma_q);
    }
    
    q2 = -1; 
    while(q2 < 0 || q2 > 1){
      q2 = rnorm(0.07, sigma_q);
    }
    
    q3 = -1; 
    while(q3 < 0 || q3 > 1){
      q3 = rnorm(0.07, sigma_q);
    }
")


# define parameters (without betas)
params_fixed <- c(gamma=1, delta1=1/(5*52),delta2=1/(55*52), alpha=1/(78.86912*52), 
                  mu=0, N=82372825, omega=1/(1*52))
# WWR's rinit
rinit <- Csnippet("
    double m = N/(S10+I10+R10+S20+I20+R20+S30+I30+R30);
    I1=nearbyint(m*I10);
    I2=nearbyint(m*I20);
    I3=nearbyint(m*I30);
    S1=nearbyint(m*S10);
    S2=nearbyint(m*S20);
    S3=nearbyint(m*S30);
    R1=nearbyint(m*R10);
    R2=nearbyint(m*R20);
    R3=nearbyint(m*R30);
    H1 = 0;
    H2 = 0;
    H3 = 0;
")

# Set to MLE
params_stocks_stst_mle <- params_fixed

params_stocks_stst_mle["beta1"] <- 11.48
params_stocks_stst_mle["beta2"] <- 0.25
params_stocks_stst_mle["beta3"] <- 0.35
params_stocks_stst_mle["phi"] <- 0.14
params_stocks_stst_mle["beta11"] <- 0.16
params_stocks_stst_mle["sigma_q"] <- 0.021
params_stocks_stst_mle["sigma_xi"] <- 66.89
params_stocks_stst_mle["S10"] <- 0.047061
params_stocks_stst_mle["I10"] <- 0.000368
params_stocks_stst_mle["R10"] <- 0.015967
params_stocks_stst_mle["S20"] <- 0.015967
params_stocks_stst_mle["I20"] <- 0.000011
params_stocks_stst_mle["R20"] <- 0.003677
params_stocks_stst_mle["S30"] <- 0.237624
params_stocks_stst_mle["I30"] <- 0.000031
params_stocks_stst_mle["R30"] <- 0.001591



pt <- pomp::parameter_trans(
  log = c("beta1","beta2","beta3","sigma_q","sigma_xi"),
  logit=c("beta11"),
  barycentric=c("S10","I10","R10",
                "S20","I20","R20",
                "S30","I30","R30"),
  toEst= pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst= pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)

read.table("real_rotavirus_metadata.txt") %>%
  rbind(data.frame(time=0,cases1=NA,cases2=NA,cases3=NA)) %>%
  arrange(time) -> dat


pomp(data = dat,
     times="time",
     t0=0,
     dmeasure = dmeas,
     rmeasure = rmeas,
     rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
     statenames = c("S1", "I1", "R1", "H1", 
                    "S2", "I2", "R2", "H2",
                    "S3", "I3", "R3", "H3", 
                    "q1", "q2", "q3"),
     paramnames = names(params_stocks_stst_mle),
     accumvars=c("H1", "H2", "H3"),
     rinit=rinit,
     partrans = pt,
     params = params_stocks_stst_mle
) -> sir

sir_panel <- panelPomp::panelPomp(list(unit1=sir),
                                  shared=NULL,
                                  specific=params_stocks_stst_mle |> 
                                    as.matrix() |>
                                    `colnames<-`("unit1")
)

require(doParallel)
cores <- ncores
registerDoParallel(cores)
mcopts <- list(set.seed=TRUE)

### Next-round code
el <- readRDS("output/round_02/ovov_02_el.rds")
x <- na.omit(el$fits)
score_total = x$logLik
ranking_total = order(score_total, decreasing = TRUE)[1:top_n_fits]

best_fits = dplyr::select(
  x[ranking_total,], -"logLik", -"se"
)

recycle_vec = sort(rep_len(1:top_n_fits, cores))
full_best_fit <- best_fits[recycle_vec, ] 

coef_names <- colnames(full_best_fit)
colnames(full_best_fit) <- gsub(".{7}$","",coef_names)

starting_values <- vector(cores, mode="list")

for(i in 1:cores){
  t(full_best_fit[i, ])  |> 
    as.matrix() |>
    `colnames<-`("unit1") -> starting_values[[i]] 
}

mifs_global <- foreach(i=1:cores,.packages='pomp', .options.multicore=mcopts) %dopar% {
  panelPomp::mif2(
    sir_panel,
    Np = NP_MIF,
    cooling.fraction.50 = 0.5,
    rw.sd = rw_sd(beta1=0.01*sd,beta2=0.01*sd,beta3=0.01*sd,
                  beta11=0.01*sd,phi=0.01*sd,sigma_q=0.01*sd,sigma_xi=0.01*sd,
                  S10=ivp(0.24)*sd,I10=ivp(0.24)*sd,R10=ivp(0.24)*sd,
                  S20=ivp(0.24)*sd,I20=ivp(0.24)*sd,R20=ivp(0.24)*sd,
                  S30=ivp(0.24)*sd,I30=ivp(0.24)*sd,R30=ivp(0.24)*sd),
    cooling.type = "geometric",
    Nmif = NMIF,
    shared.start = numeric(0),
    specific.start = starting_values[[i]],
    block = F
  ) 
}

bake(file = "output/round_03/ovov_mif_03.rds",{ 
  mifs_global
})

bake(file = "output/round_03/ovov_03_el.rds",{
  el <- measlespkg::eval_logLik(mifs_global, ncores = cores, np_pf = NP_MIF, nreps=cores)
  el
})
@

\section{Introduction}

\ed{WE WILL NEED TO WORD THIS CAREFULLY. WE MUST BE FORCEFUL ABOUT THE VALUE OF CORRECTING THE SCIENTIFIC RECORD, WITHOUT SEEMING UNNECESSARILY CONFRONTATIONAL. ONE WAY TO DO THIS IS TO BE COMPLIMENTARY ABOUT PAL AS MUCH AS POSSIBLE. AFTER ALL, IF THE METHOD HAS SOME GOOD PROPERTIES THEN THIS ADDS VALUE TO UNDERSTANDING PROPERLY ITS LIMITATIONS. I'VE REWRITTEN THE ABSTRACT FROM THAT PERSPECTIVE.}

\ed{AT THIS POINT, THE DRAFT IS QUITE PROVISIONAL, PUTTING THINGS IN PLACE SO IT CAN BE REFINED LATER.}



\cite{wwr} claimed that PAL is the first likelihood-based filtering method that can be applied to a broad range of POMP models with the finite-population assumption and consistent results, and requires much less computational time than SMC. To evaluate whether SMC performs as well as PAL in terms of both computational time and likelihood identifiability, we replicated the model using the R package \textbf{pomp} \citep{pomppackagepaper}. First, we verified that the two models are essentially identical. Here, we simulated data from the model 1000 times at the MLE shown in Table \ref{tab:mlebywwr} and compared the summary statistics of 1000 simulated time series from two models using the t-test. Summary statistics including the mean, median, and variance for all three age groups were tested. The results showed no significant differences between the two models from which the data were simulated, confirming that both the latent process model and the observation model are essentially consistent with each other, given that the observations were directly simulated. These results are presented in Appendix \ref{appendA}.

\begin{table}[htbp]
  \centering
  \begin{threeparttable}
    \caption{Maximum likelihood estimation of parameters by PAL for three models \citep{wwr}, and by mif2 algorithm \citep{pomppackagepaper} for the OvOv model.}
    \label{tab:mlebywwr}
    \begin{tabular}{>{\raggedright\arraybackslash}p{2cm} *{3}{>{\centering\arraybackslash}p{1.5cm}} >{\centering\arraybackslash}p{2.5cm}}
      \toprule
      Parameter      & EqEq  & EqOv  & OvOv  & OvOv (\textbf{pomp})$^*$ \\
      \midrule
      \(\beta_1\)    & 12.15 & 12.74 & 11.48 & $\Sexpr{'test'}$\\
      \(\beta_2\)    & 0.22  & 0.21  & 0.25  & $\Sexpr{'test'}$\\
      \(\beta_3\)    & 0.34  & 0.31  & 0.35  & $\Sexpr{'test'}$\\
      \(\phi\)       & 0.017 & 0.14  & 0.14  & $\Sexpr{'test'}$\\
      \(\rho\)       & 0.022 & 0.19  & 0.16  & $\Sexpr{'test'}$\\
      \(\sigma^2_q\) & n/a   & 0.042 & 0.021 & $\Sexpr{'test'}$\\
      \(\sigma_\xi\) & n/a   & n/a   & 66.89 & $\Sexpr{'test'}$\\
      \addlinespace
      \multicolumn{1}{l}{\(S_{10}\)} & \multicolumn{3}{c}{$3876549^*$} & $5885201$\\
      \multicolumn{1}{l}{\(I_{10}\)} & \multicolumn{3}{c}{$30351$}     & $5041$\\
      \multicolumn{1}{l}{\(R_{10}\)} & \multicolumn{3}{c}{$1315221$}   & $721006$\\
      \multicolumn{1}{l}{\(S_{20}\)} & \multicolumn{3}{c}{$57139612$}  & $44546887$\\
      \multicolumn{1}{l}{\(I_{20}\)} & \multicolumn{3}{c}{$871$}       & $22$\\
      \multicolumn{1}{l}{\(R_{20}\)} & \multicolumn{3}{c}{$302852$}    & $26360811$\\
      \multicolumn{1}{l}{\(S_{30}\)} & \multicolumn{3}{c}{$19573727$}  & $4788420$\\
      \multicolumn{1}{l}{\(I_{30}\)} & \multicolumn{3}{c}{$2550$}      & $131$\\
      \multicolumn{1}{l}{\(R_{30}\)} & \multicolumn{3}{c}{$131092$}    & $65306$\\
      \midrule
      \addlinespace
      \textbf{AIC}  & $98866.65$ & $15154.75$ & $13778.08$ & $\Sexpr{'test'}$\\
      \addlinespace
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \small
      \item[*] The initial distribution parameter \(\lambda_0 = (S_{10}, I_{10}, R_{10}, S_{20}, I_{20}, R_{20}, S_{30}, I_{30}, R_{30})\) are assumed to be \textbf{fixed} in \cite{wwr}. The results of the estimates of the OvOv (\textbf{pomp}) model are obtained by using the Iterated Filtering algorithm with 3 rounds and 100 iterations, 50,000 particles, and 36 replicates in each round with the top 12 best fits in terms of likelihood chosen to be the starting value for the next round. 
    \end{tablenotes}
  \end{threeparttable}
\end{table}



\subsection{Key Factors Contributing to the Change of AIC in Table \ref{tab:model_performance}}\label{section1}

From Table \ref{tab:model_performance}, it is evident that the OvOv model yields the lowest AIC, making it particularly noteworthy. Since one of \cite{stocks}'s main purposes was to do model selection, therefore it's exciting to see \cite{wwr} obtaining a much better model. The overdispersion model (OvOv) is often the best model in terms of likelihood, primarily because it allows for greater variation in the model. By incorporating more parameters, the OvOv model can capture subtleties and complexities in the dataset that simpler models may overlook, leading to a more accurate fit and higher likelihood values. \cite{wwr} claimed that it is the innovative observation model that contributed to an increase in log-likelihood by 3200 log-likelihood units. However, after comparing the results, we found that the increase in likelihood when implementing the OvOv model is attributable to the datasets used by \cite{wwr} and \cite{stocks}, where \cite{stocks}'s dataset has no underreporting. Therefore \cite{wwr}'s dataset can be viewed as a scaled-down version of \cite{stocks}'s dataset by the reporting rate, which is roughly 0.07. Within the framework of the same model, scaling the data downwards by a factor results in a corresponding rise in likelihood. This scaling effect accounts for the discrepancy observed in the increase of 3200 log-likelihood units.
\vspace{-2mm}
\subsection{PAL versus SMC: When the Model is Misspecified} \label{misspecified}

Our results demonstrate how SMC is sensitive to model misspecification, which consequently leads to a low likelihood estimate at the MLE, as shown in Table \ref{tab:ovovrealdata} by fitting the real German rotavirus dataset into the model.

\begin{table}[ht] % The placement specifier can be [h!tbp]
\centering % Centers the table
\caption{log-likelihood at the MLE of OvOv model for the real rotavirus data, computed using two filtering methods, employed 36 replicates and 50,000 particles} % This is effectively the table title
\label{tab:ovovrealdata} % For referencing this table elsewhere in your text
\begin{tabular}{|c c c|} 
 \hline
  & PAL & SMC  \\
  \hline
 OvOv & -6892.168 & -7200.866 \\ 
 \hline
\end{tabular}
\end{table}
\vspace{-3mm}

Table \ref{tab:ovovrealdata} indicates that the results from the two methods are quite similar but SMC obtained a bit lower log-likelihood estimate than PAL. One potential explanation is the model misspecification, leading to a sensitive detection of likelihood shortfalls for SMC. Since consistent datasets, when faced with model misspecification due to outliers in the data, challenge the SMC approach, leading to low weights being assigned to almost all particles (recall Algorithm \ref{alg1}), as the latent process model struggles to accommodate outliers and is misspecified for such data points, which results in low conditional likelihood at that specific data point. Moreover, for SMC, a situation where a few particles acquire a high weight while the majority are assigned negligible weights can lead to an imbalance in the resampling process. This scenario might result in these few heavily weighted particles being resampled repeatedly, thereby dominating the sample set. If, following this, the subsequent simulations are unable to track the model accurately, it is regarded as an SMC failure. However, such an occurrence was not observed in our analysis, indicating that the SMC method functioned without encountering this specific type of failure.

To examine the model misspecification at potential time points, as depicted in Figure \ref{fig:cond_SMC_PAL_realdat}, the majority of significant drops of the SMC occurred at time ($t=1, \,\,2, \,\,3, \,\,11, \,\,81, \,\,194, \,\,325-333$). The significant drops observed initially, specifically at times $t=1, 2, 3$, may suggest an improper initial distribution from which the first set of particles was simulated. At times $t=11, \,\,81, \,\,194$ and $t=325-333$, the shortfall approximated $50-100$ log-units. These failures can be attributed to outliers that the model cannot explain, as shown in Appendix \ref{appendB}. This suggests model misspecification, particularly in the latent process, which fails to generate suitable particles capable of producing a high likelihood of observing the true reported cases from the data. Subsequent issues observed at later times could indicate problems caused by fixed seasonality, which cannot be compensated for by adding more stochastic elements. 

\vspace{-4.5mm}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{thesis-figs/cond_SMC_PAL_realdat.png}
\caption{\label{fig:cond_SMC_PAL_realdat}Conditional log-likelihoods computed using two methods for the real rotavirus dataset, with the SMC method replicated 36 times due to its high variances. The blue line represents overlapping results from 36 SMC computations, while the red line is derived from PAL. The main sources of likelihood shortfall of SMC are at time points $t=1, 2, 3, 11, 81, 194$, and $325-333$.}
\end{figure}

\vspace{-8mm}
\subsection{PAL versus SMC: When the Model is Correctly Specified}\label{section3}

Here, we utilize 100 simulated datasets from the OvOv model with parameters set at the MLE, thereby eliminating model misspecification. We then examine the likelihood values computed by PAL and SMC.


\begin{table}[htbp] % The placement specifier can be [h!tbp]
\centering % Centers the table
\caption{Average log-likelihood at the MLE at the 100 simulated datasets from OvOv computed by two filtering methods, employed 36 replicates and 50,000 particles} % This is effectively the table title
\label{tab:ovovtrue} % For referencing this table elsewhere in your text
\begin{tabular}{|c c c|} 
 \hline
  & PAL & SMC  \\ 
 \hline
 OvOv & -6231.97 & -6224.245 \\ 
 \hline
\end{tabular}
\end{table}
\begin{figure}[H]
\centering
\vspace{-7mm}
\includegraphics[width=0.55\textwidth]{thesis-figs/PAL_vs_SMC_ovov_100.png}
\caption{\label{fig:PAL_vs_SMC_ovov_100}Log-likelihoods computed using two filtering methods for 100 randomly simulated datasets at the MLE of the OvOv model. The red line is the line $Lik(pfilter)=Lik(palfilter)$. We can see on the simulated data, SMC gives consistently higher likelihood estimates than PAL.}
\end{figure}



When the model is correctly specified, however, the log-likelihood values computed by PAL, as shown in Table \ref{tab:ovovtrue} and Figure \ref{fig:PAL_vs_SMC_ovov_100}, consistently exhibit a shortfall. This shortfall highlights the approximative nature of the PAL method compared to the more accurate SMC filter. It is posited that the model yielding the highest likelihood is, in theory, the true model, a concept we will explore in further detail in the discussion section \ref{dis}.

\subsection{Iterated Filtering Maximization of the Likelihood}

Besides proposing a novel filtering algorithm, by applying PAL to the model for rotavirus, \cite{wwr} claimed they have also identified a superior model, as a much lower AIC represented in Table \ref{tab:model_performance} indicates and the failure from getting a lower likelihood in Section \ref{misspecified}. However, as demonstrated in \ref{section3}, we recognize that SMC, functioning as an ideal filter, ought to yield a higher likelihood estimation when the model is correct. Notably, Section \ref{misspecified} reveals that when the model confronts actual rotavirus data, it encounters misspecification issues, particularly at times $t=1, \,\,2, \,\,3, \,\,11, \,\,81, \,\,194, \,\,325-333$, leading to shortfalls in likelihood. Appendix \ref{appendB} discusses this phenomenon, attributing it to certain data points that the model fails to adequately represent. This issue is especially pronounced at the initial time points. Particularly, in the initial time points $t=1,2,3$, the likelihood deviates from PAL estimates for about more than 100 log-likelihood units, suggesting an improper choice of initial distribution that might favor the PAL estimate. Consequently, we have opted to estimate the initial distribution \(\lambda_0 = (S_{10}, I_{10}, R_{10}, S_{20}, I_{20}, R_{20}, S_{30}, I_{30}, R_{30})\), where $S_{10}, I_{10}, R_{10}, S_{20}, I_{20}, R_{20}, S_{30}, I_{30}$, and $R_{30}$ represent the number of individuals in each compartment at time $t=0$. The results of this new estimation can be found in Table \ref{tab:mlebywwr}. Importantly, the maximization of the likelihood conducted via the Iterated Filtering (mif2) algorithm \citep{pomppackagepaper} procures a higher likelihood at the estimated MLE than that estimated by PAL, and a different yet comparable set of estimates is obtained. In terms of parameters pertinent to epidemiological interests, i.e. the first 7 rows in Table \ref{tab:mlebywwr}, we observe a lower force of infection parameter for the first age group, $\beta_1$, alongside a higher force of infection for the third age group, $\beta_3$. Additionally, we detect variations in the phase parameter $\phi$, which directly affects the seasonal time points, as well as in the stochastic parameter $\sigma_{\xi}$ and the initial compartmental distributions. The modifications to the initial distributions aim to rectify inadequate initialization, while the other adjustments address model misspecifications identified in Section \ref{misspecified}. Changes in the remaining parameter set are negligible. These findings indicate that when model misspecification is reduced to a minimal level, both PAL and SMC approaches tend to provide similar likelihood estimates, and the Iterated Filtering algorithm is capable of generating a superior maximum likelihood estimation than PAL equipped with the gradient ascent algorithm. Moreover, the results produced by SMC are more reliable due to the exact nature of the filter, despite PAL's computational efficiency.


\section{Discussion and Conclusion}\label{dis}

This paper examines two filtering approaches, with PAL specifically designed for certain types of compartmental POMP models. However, both methods have their limitations. Although this was not observed in our study, SMC is sensitive to model misspecification, which can lead to filtering failures. Conversely, PAL may experience a shortfall in likelihood estimation when the model is correctly specified.

PAL is an approximate filtering method used in scenarios where SMC is computationally prohibitive. It approximates the posterior distribution by a Poisson approximation. Due to its approximative nature, it does not perfectly capture the true posterior distribution. 
SMC, however, often referred to as the "perfect filter," uses a large set of particles and sophisticated resampling techniques to theoretically accurately approximate the posterior distribution. It is computationally more intensive than PAL but tends to provide a satisfying approximation, especially in complex models that are not compatible with PAL in general. When PAL and SMC are applied to a well-specified model meaning the model with the latent process model being correctly represented, SMC outperforms PAL because it better captures the nuances of the posterior distribution. This is why SMC might show a "likelihood shortfall" compared to PAL when the model is correctly specified. This is due to the approximation error of PAL can't be improved arbitrarily in general \citep{annurev:/content/journals/10.1146/annurev-control-042920-015119}, and likelihood being a proper scoring rule.

The concept of likelihood as a proper scoring rule \citep{gneiting1,gneiting2} in statistical models implies that using likelihood-based methods will naturally align with the true model of the data, assuming the model structure correctly represents the underlying process. When the likelihood function is used as a scoring rule, it is considered proper because maximizing the likelihood leads to parameter estimates that converge to their true values under large sample conditions, provided the model is specified correctly. A scoring rule is "strictly proper" if a forecaster maximizes the expected score by issuing a probabilistic forecast that exactly matches the true distribution of the outcomes. In other words, for a scoring rule to be strictly proper, the best strategy for the forecaster is to report the true probability distribution of the events they are forecasting. This promotes honest and accurate forecasting. This provides a theoretical explanation of why SMC outperforms PAL when the model is correct.

The "likelihood shortfall" in PAL when compared to SMC, particularly when the model is correct, highlights the trade-off between computational efficiency and accuracy in statistical approximations. While PAL may be faster and more computationally feasible in many scenarios, it sacrifices some accuracy, which becomes apparent when compared to the more exact SMC method, especially under the scrutiny of proper scoring rules as discussed in the context.


\section{Model Misspecification}\label{appendB}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{thesis-figs/plot_zoom_png.png}
\caption{\label{fig:zoomedin_outlier_plot} Zoomed-in time-Cases plot at time $t=0-20, 75-85, 190-200$ and $320-340$. In the visualization, solid lines represent the true reported cases from the original rotavirus dataset, while the shaded strips with dashed boundaries-colored according to different groups—indicate enlarged "confidence intervals" for these cases. Specifically, the upper boundary of each strip corresponds to the upper 97.5\% percentile of 36 replicates of the filtered accumulated number of cases, $H_{kt}$, for $k=1,2,3$, and $t=1,2,...,416$, as introduced in Section \ref{observationmodel}. This boundary is further adjusted by the upper 97.5\% percentile of the reporting rate $q_t$, estimated by \cite{wwr} in Table \ref{tab:mlebywwr}. The reporting rate $q_t$ is assumed to follow a truncated normal distribution with a mean of 0.07 and a standard deviation of $\sigma_q = 0.021$, resulting in a lower 2.5\% percentile of 0.029 and an upper 97.5\% percentile of 0.111. The methodology for establishing the lower boundary of the strip mirrors that of the upper boundary. Although this approach is not entirely rigorous, each strip is intended to encompass our extended confidence interval for the true reported cases. Consequently, data points that fall outside these strips give us a intuition of where the model misspecifications may occur.}
\end{figure}

As illustrated in Figure \ref{fig:cond_SMC_PAL_realdat}, shortfalls are observed at times $t=1,\,\,2,\,\,3,\,\,11,\,\,81,\,\,194,$ and $325-333$. These discrepancies are primarily attributable to outliers that the model fails to accommodate. In the displayed results, it is immediately apparent that at the initial times $t=1, \,\,2, \,\,3$ all the true reported cases deviated from the expected range, falling outside the designated strip. Specifically, at time $t=11$, the model is misspecified for case 1, depicted by the solid red line. Similarly, at times $t=81,\,\, 82$, the model is misspecified for case 3, as indicated by the solid blue line that extended beyond the strip's boundaries. At $t=194$, the solid green line denotes case 2 as where the model is misspecified. During the period from $t=325-333$, all three observed cases are where the model misspecifications happen. These deviations provide a clear illustration of the likelihood shortfall of the sequential Monte Carlo (SMC) method when the model does not align with the data, suggesting a model misspecification. 

\bibliography{bib-pal}

\end{document}
