\documentclass[10pt]{article}
\usepackage{color}
\usepackage{graphicx} % Required for inserting
\usepackage{booktabs} % For better-quality horizontal lines
\usepackage{multirow}
\usepackage{makecell}
% \usepackage{threeparttable} % For table notes
\usepackage{array} % For table formatting
\usepackage{subcaption}
\usepackage{setspace} % For double-line spacing
\usepackage{float} 
\usepackage{tabularx}
\usepackage[round]{natbib}
\usepackage[font={small,it}]{caption}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{amsfonts,amssymb,amsthm}
\usepackage{url}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}
% Using \doublespacing in the preamble 
% changes the text to double-line spacing
% \doublespacing

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\newcommand{\ed}[1]{\textcolor{red}{[EI: #1]}}
\newcommand{\yize}[1]{\textcolor{green}{[YH: #1]}}
\newcommand{\aaron}[1]{\textcolor{blue}{[AA: #1]}}

\bibliographystyle{apalike}

\begin{document}

\title{Poisson approximate likelihood compared to the particle filter}
\author{Yize Hao, Aaron A. Abkemeier and Edward L. Ionides}
\date{Draft compiled on \today}
\maketitle

\begin{abstract}
Filtering algorithms are fundamental for inference on partially observed stochastic dynamic systems, since they provide access to the likelihood function and hence enable likelihood-based or Bayesian inference.
A novel Poisson approximate likelihood (PAL) filter was introduced by \cite{wwr}.
PAL employs a Poisson approximation to conditional densities, offering a fast approximation to the likelihood function for a certain subset of partially observed Markov process models.
A central piece of evidence for PAL is the comparison in Table~1 of \cite{wwr}, which claims a large improvement for PAL over a standard particle filter algorithm.
This evidence, based on a model and data from a previous scientific study by \citet{stocks}, might suggests that researchers confronted with similar models should use PAL rather than previous particle filter methods.
Alternatively, the improvement in likelihood found by \cite{wwr} compared to \cite{stocks} might indicate a flaw in the model or numerical methods for that previous work.
We show that neither of these conclusions is valid, and the comparison of log-likelihood values made by \cite{wwr} is flawed because the PAL calculations were carried out using a dataset scaled differently from the previous study.
If PAL and the particle filter are used on this new scale, the superficial advantage of PAL largely disappears.
On simulations where the model is correctly specified, the particle filter outperforms PAL.
If a poorly fitting specification of initial value parameters is amended, the particle filter also outperforms PAL on the actual data.
\end{abstract}


% R starts here

<<setup,echo=FALSE,message=FALSE,warning=FALSE>>=
rm(list = ls())     # clear objects
library("knitr")
opts_knit$set(concordance=TRUE)
opts_chunk$set(
  tidy = TRUE,
# cache = TRUE,
  cache = FALSE,  # set cache = FALSE when editing the code
# message = FALSE,
# warning = FALSE,
  tidy.opts = list(
    keep.blank.line = FALSE
  ),
  comment = "",
  echo = FALSE,
  results = FALSE,
  dev.args = list(
    bg = "transparent",
    pointsize = 9
  ),
  fig.path = "figure/"
)

myround <- function(x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

graphics.off()      # close graphics windows
library(pomp)
library(ggplot2)
library(scales)
library(foreach)
library(doParallel)
library(doFuture)
library(dplyr)
library(Rcpp)
library(RcppArmadillo)
sourceCpp('rotavirus_normq.cpp')
source("choose_n_fits.R")
source("eval_logLik_pomp.R")

RUN_LEVEL = 3
NP_MIF       = switch(RUN_LEVEL, 10,   5000,  50000)
NMIF         = switch(RUN_LEVEL, 5,    20,    100)
ncores       = switch(RUN_LEVEL, 4,    10,    36)
top_n_fits   = switch(RUN_LEVEL, 1,    2,     12)
sim_rep      = switch(RUN_LEVEL, 5,    20,    100) #sim_ovov

#population number
pop <- 82372825
time <- c(1:416)

out_dir <- paste0("output_0",RUN_LEVEL, "/")
@


% OvOv model is here

<<OvOv-model, echo=F, warning=F, message=F>>=
# measurement model 
dmeas <- Csnippet("
  lik =  dbinom(cases1, H1, q1, 1) +
  dbinom(cases2, H2, q2, 1) +
  dbinom(cases3, H3, q3, 1);"
)
rmeas <-  Csnippet("
  cases1 = rbinom(H1, q1);
  cases2 = rbinom(H2, q2);
  cases3 = rbinom(H3, q3);
")

rproc <- Csnippet("
  int I = I1 + I2 + I3;
  int trans_S1[3], trans_S2[3], trans_S3[2], trans_I1[3], trans_I2[3], 
  trans_I3[2], trans_R1[3], trans_R2[3], trans_R3[2];
    
  double prob_S1[3], prob_I1[3], prob_R1[3], prob_S2[3], prob_I2[3], prob_R2[3],
  prob_S3[2], prob_I3[2], prob_R3[2];
    
  double xi = rgamma(sigma_xi, 1/sigma_xi);
    
  double kappa = (1 + beta11*cos(2*3.141593*t/52 + phi)) * xi;
    
  // Define rate
  prob_S1[0] = 1 - exp(-dt*beta1*kappa*I/N); // 0->1
  prob_S1[1] = 1 - exp(-delta1*dt);
  prob_S1[2] = exp(-delta1*dt) + exp(-dt*beta1*kappa*I/N) - 1;
    
  prob_I1[0] = 1 - exp(-gamma*dt);
  prob_I1[1] = 1 - exp(-delta1*dt);
  prob_I1[2] = exp(-gamma*dt)+exp(-delta1*dt) - 1;
    
  prob_R1[0] = 1 - exp(-omega*dt);  // E_1,t this goes back to S_1,(t+1)
  prob_R1[1] = 1 - exp(-delta1*dt);
  prob_R1[2] = exp(-omega*dt) + exp(-delta1*dt) - 1;
    
  prob_S2[0] = 1 - exp(-dt*beta2*kappa*I/N);
  prob_S2[1] = 1 - exp(-delta2*dt);
  prob_S2[2] = exp(-delta2*dt) + exp(-dt*beta2*kappa*I/N) - 1;
    
  prob_I2[0] = 1 - exp(-dt*gamma);
  prob_I2[1] = 1 - exp(-dt*delta2);
  prob_I2[2] = exp(-dt*gamma)+exp(-dt*delta2) - 1;
    
  prob_R2[0] = 1 - exp(-dt*omega);  // E_1,t this goes back to S_1,(t+1)
  prob_R2[1] = 1 - exp(-dt*delta2);
  prob_R2[2] = exp(-dt*omega) + exp(-dt*delta2) - 1;
    
  // For Age Group (3): Die first before transition;
    
  int S3mD, I3mD, R3mD;
    
  S3mD = rbinom(S3, 1-dt*mu); // S3 minus Death: mu is the death rate, so it's 1-mu here
  I3mD = rbinom(I3, 1-dt*mu);
  R3mD = rbinom(R3, 1-dt*mu);
    
  prob_S3[0] = 1-exp(-dt*beta3*kappa*I/N);
  prob_S3[1] = exp(-dt*beta3*kappa*I/N);
    
  prob_I3[0] = 1 - exp(-dt*gamma);
  prob_I3[1] = exp(-dt*gamma);
    
  prob_R3[0] = 1 - exp(-dt*omega);
  prob_R3[1] = exp(-dt*omega);
    
  // Transition
  // B: S->I
  // C: I->R
  // F: Aging: (1)->(2)->(3)
  // E: R->S
  // D: Death
  //// Note: Here S_1, S_2... are all old values from (t-1)
  rmultinom(S1, &prob_S1, 3, &trans_S1); // B, F, S-B-F
  rmultinom(I1, &prob_I1, 3, &trans_I1); // C, F, I-C-F
  rmultinom(R1, &prob_R1, 3, &trans_R1); // E, F, R-E-F
    
  rmultinom(S2, &prob_S2, 3, &trans_S2); // B, F, S-B-F
  rmultinom(I2, &prob_I2, 3, &trans_I2); // C, F, I-C-F
  rmultinom(R2, &prob_R2, 3, &trans_R2); // E, F, R-E-F
    
  rmultinom(S3mD, &prob_S3, 2, &trans_S3); // B, (S-D)-B
  rmultinom(I3mD, &prob_I3, 2, &trans_I3); // C, (I-D)-C
  rmultinom(R3mD, &prob_R3, 2, &trans_R3); // E, (R-D)-E
    
  S1 = trans_S1[2] + trans_R1[0] + rpois(4*1025.7); // Include Birth
  I1 = trans_I1[2] + trans_S1[0];
  R1 = trans_R1[2] + trans_I1[0];
    
  S2 = trans_S2[2] + trans_R2[0] + trans_S1[1]; // Include Aging
  I2 = trans_I2[2] + trans_S2[0] + trans_I1[1];
  R2 = trans_R2[2] + trans_I2[0] + trans_R1[1];
    
  S3 = trans_S3[1] + trans_R3[0] + trans_S2[1]; // Include Aging
  I3 = trans_I3[1] + trans_S3[0] + trans_I2[1];
  R3 = trans_R3[1] + trans_I3[0] + trans_R2[1];
    
  //Accumvar
  H1 += trans_S1[0];
  H2 += trans_S2[0];
  H3 += trans_S3[0];
    
  q1 = -1; 
  while(q1 < 0 || q1 > 1){
    q1 = rnorm(0.07, sigma_q);
  }
    
  q2 = -1; 
  while(q2 < 0 || q2 > 1){
    q2 = rnorm(0.07, sigma_q);
  }
    
  q3 = -1; 
  while(q3 < 0 || q3 > 1){
    q3 = rnorm(0.07, sigma_q);
  }
")


# full-estimate-rinit
rinit_full <- Csnippet("
  double m = N/(S10+I10+R10+S20+I20+R20+S30+I30+R30);
  I1=nearbyint(m*I10);
  I2=nearbyint(m*I20);
  I3=nearbyint(m*I30);
  S1=nearbyint(m*S10);
  S2=nearbyint(m*S20);
  S3=nearbyint(m*S30);
  R1=nearbyint(m*R10);
  R2=nearbyint(m*R20);
  R3=nearbyint(m*R30);
  H1 = 0;
  H2 = 0;
  H3 = 0;
")

# WWR's rinit
rinit_wwr <- Csnippet("
  S1=3876549;
  S2=57139612;
  S3=19573727;
  I1=30351;
  I2=871;
  I3=2550;
  R1=1315221;
  R2=302852;
  R3=131092;
  H1 = 0;
  H2 = 0;
  H3 = 0;
")

rinit_stocks <- Csnippet("
  S1=5.09484e+06;
  S2=5.73856e+07;
  S3=1.96976e+07;
  I1=2871;
  I2=639;
  I3=174;
  R1=124410;
  R2=57072;
  R3=9578;
  H1 = 0;
  H2 = 0;
  H3 = 0;
")

# x <- c(3876549,57139612,19573727,30351,871,2550,1315221,302852,131092)
# x/sum(x)

# define parameters (fixed)
params_fixed <- c(
  gamma = 1, 
  delta1 = 1/(5*52),
  delta2 = 1/(55*52), 
  alpha = 1/(78.86912*52), 
  mu = 0, 
  N = 82372825, 
  omega = 1/(1*52)
)
# Set to MLE
params_wwr <- c(
  params_fixed, 
  "beta1" = 11.48,
  "beta2" = 0.25,
  "beta3" = 0.35,
  "phi" = 0.14,
  "beta11" = 0.16,
  "sigma_q" = 0.021,
  "sigma_xi" = 66.89
)

params_full <- c(
  params_wwr,
  "S10" = 4.706102e-02,
  "S20" = 6.936707e-01,
  "S30" = 2.376236e-01,
  "I10" = 3.684589e-04,
  "I20" = 1.057388e-05,
  "I30" = 3.095681e-05,
  "R10" = 1.596669e-02,
  "R20" = 3.676601e-03,
  "R30" = 1.591447e-03
)

pt_full <- pomp::parameter_trans(
  log = c("beta1", "beta2", "beta3", "sigma_q", "sigma_xi"),
  logit = c("beta11"),
  barycentric = c("S10","I10","R10",
                  "S20","I20","R20",
                  "S30","I30","R30"),
  toEst = pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst = pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)

pt_wwr <- pomp::parameter_trans(
  log = c("beta1", "beta2", "beta3", "sigma_q", "sigma_xi"),
  logit = c("beta11"),
  toEst = pomp::Csnippet("T_phi = logit(phi/(M_2PI));"),
  fromEst = pomp::Csnippet("phi = M_2PI*expit(T_phi);")
)


dat <- read.table("real_rotavirus_metadata.txt") |>
  arrange(time)

statenames <- c(
  "S1", "I1", "R1", "H1",
  "S2", "I2", "R2", "H2",
  "S3", "I3", "R3", "H3",
  "q1", "q2", "q3"
)

sir_wwr <- pomp(
  data = dat,
  times = "time",
  t0 = 0,
  dmeasure = dmeas,
  rmeasure = rmeas,
  rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
  statenames = statenames,
  paramnames = names(params_wwr),
  accumvars = c("H1", "H2", "H3"),
  rinit = rinit_wwr,
  partrans = pt_wwr,
  params = params_wwr
) 

sir_full <- pomp(
  sir_wwr,
  rinit = rinit_full,
  params = params_full,
  partrans = pt_full,
  statenames = statenames,
  paramnames = names(params_full)
)
@


% Maximization Round 1 - Full rinit

<<maximization1, echo=F, warning=F, message=F>>=
set.seed(123)
sir_box <- list(
  beta1 = c(10, 15),
  beta2 = c(0.2, 0.4),
  beta3 = c(0.3, 0.5),
  phi = c(0.01, 0.3),
  beta11 = c(0.1, 0.2),
  sigma_q = c(0.001, 0.1),
  sigma_xi = c(65, 70),
  S10 = c(0.01, 0.05),
  I10 = c(0.0001, 0.0005),
  R10 = c(0.01, 0.05),
  S20 = c(0.01, 0.7),
  I20 = c(0.000001, 0.00002),
  R20 = c(0.001, 0.005),
  S30 = c(0.01, 0.5),
  I30 = c(0.000001, 0.0001),
  R30 = c(0.0001, 0.005),
  gamma = c(1, 1),
  delta1 = c(1/(5*52), 1/(5*52)),
  delta2 = c(1/(55*52), 1/(55*52)),
  alpha = c(1/(78.86912*52), 1/(78.86912*52)),
  mu = c(0, 0),
  N = c(82372825, 82372825),
  omega = c(1/(1*52), 1/(1*52))
)

design_matrix <- runif_design(
  lower = sapply(sir_box, `[`, 1),
  upper = sapply(sir_box, `[`, 2),
  n = ncores
)

bake(file = paste0(out_dir, "mif2_maximization/full/round_01/mifs_global_01.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      sir_full,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(
        beta1 = 0.01, beta2 = 0.01, beta3 = 0.01, beta11 = 0.01, 
        phi = 0.01, sigma_q = 0.01, sigma_xi = 0.01,
        S10 = ivp(0.24), I10 = ivp(0.24), R10 = ivp(0.24),
        S20 = ivp(0.24), I20 = ivp(0.24), R20 = ivp(0.24),
        S30 = ivp(0.24), I30 = ivp(0.24), R30 = ivp(0.24)
      ),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = design_matrix[i, ]
    ) 
  }
}) -> mifs_global_01 


bake(file = paste0(out_dir, "mif2_maximization/full/round_01/el_01.rds"),{
  eval_logLik_pomp(mifs_global_01, ncores, NP_MIF, 4248930)
}) ->  el_01

@

% Continue: Maximization Round 2 - Full rinit

<<maximization2, echo=F, warning=F, message=F>>=
set.seed(1234)
sd_02 = 2/3
starting_values_02 <- choose_n_fits(el_01, top_n_fits, ncores)

bake(file = paste0(out_dir,"mif2_maximization/full/round_02/mifs_global_02.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_01[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_02, beta2 = 0.01*sd_02, beta3 = 0.01*sd_02,
        beta11 = 0.01*sd_02, phi = 0.01*sd_02,
        sigma_q = 0.01*sd_02, sigma_xi = 0.01*sd_02,
        S10 = ivp(0.24)*sd_02, I10 = ivp(0.24)*sd_02, R10 = ivp(0.24)*sd_02,
        S20 = ivp(0.24)*sd_02, I20 = ivp(0.24)*sd_02, R20 = ivp(0.24)*sd_02,
        S30 = ivp(0.24)*sd_02, I30 = ivp(0.24)*sd_02, R30 = ivp(0.24)*sd_02
      ),
      params = starting_values_02[[i]]
    ) 
  }
}) ->  mifs_global_02


bake(file = paste0(out_dir, "mif2_maximization/full/round_02/el_02.rds"),{
  eval_logLik_pomp(mifs_global_02, ncores, NP_MIF, seed=4248930)
}) ->  el_02
@

% Continue: Maximization Round 3 - Full rinit

<<maximization3, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_03 = (2/3)^2
starting_values_03 <- choose_n_fits(el_02, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/full/round_03/mifs_global_03.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_02[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_03, beta2 = 0.01*sd_03, beta3 = 0.01*sd_03,
        beta11 = 0.01*sd_03, phi = 0.01*sd_03,
        sigma_q = 0.01*sd_03, sigma_xi = 0.01*sd_03,
        S10 = ivp(0.24)*sd_03, I10 = ivp(0.24)*sd_03, R10 = ivp(0.24)*sd_03,
        S20 = ivp(0.24)*sd_03, I20 = ivp(0.24)*sd_03, R20 = ivp(0.24)*sd_03,
        S30 = ivp(0.24)*sd_03, I30 = ivp(0.24)*sd_03, R30 = ivp(0.24)*sd_03
      ),
      params = starting_values_03[[i]]
    ) 
  }
}) -> mifs_global_03


bake(file = paste0(out_dir,"mif2_maximization/full/round_03/el_03.rds"),{
  eval_logLik_pomp(mifs_global_03, ncores, NP_MIF, seed = 4248930)
}) ->  el_03
@

<<estimated-mle, echo=FALSE, message=FALSE, warning=FALSE>>=
# Can be changed if more rounds needed
est_mle_vec <- el_03[which.max(el_03$logLik),]
@


% Maximization Round 1 - Stocks rinit

<<maximization1-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(123)

dat %>%
  rbind(data.frame(time=0,cases1=NA,cases2=NA,cases3=NA)) %>%
  arrange(time) -> dat_sbh

sir_sbh_init <- pomp(
  sir_wwr,
  data = dat_sbh,
  times = "time",
  t0 = 1-6*52,
  rinit = rinit_stocks,
  statenames = statenames
) 

sir_box_sbh <- list(
  beta1 = c(10, 15),
  beta2 = c(0.2, 0.4),
  beta3 = c(0.3, 0.5),
  phi = c(0.01, 0.3),
  beta11 = c(0.1, 0.2),
  sigma_q = c(0.001, 0.1),
  sigma_xi = c(65, 70),
  gamma = c(1, 1),
  delta1 = c(1/(5*52), 1/(5*52)),
  delta2 = c(1/(55*52), 1/(55*52)),
  alpha = c(1/(78.86912*52), 1/(78.86912*52)),
  mu = c(0, 0),
  N = c(82372825, 82372825),
  omega = c(1/(1*52), 1/(1*52))
)

design_matrix_sbh <- runif_design(
  lower = sapply(sir_box_sbh, `[`, 1),
  upper = sapply(sir_box_sbh, `[`, 2),
  n = ncores
)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_01/mifs_global_sbh_01.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      sir_sbh_init,
      Np = NP_MIF,
      cooling.fraction.50 = 0.5,
      rw.sd = rw_sd(
        beta1 = 0.01, beta2 = 0.01, beta3 = 0.01, beta11 = 0.01, 
        phi = 0.01, sigma_q = 0.01, sigma_xi = 0.01
      ),
      cooling.type = "geometric",
      Nmif = NMIF,
      params = design_matrix_sbh[i, ]
    ) 
  }
}) -> mifs_global_sbh_01 


bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_01/sbh_el_01.rds"),{
  eval_logLik_pomp(mifs_global_sbh_01, ncores, NP_MIF, 4248930)
}) -> sbh_el_01
@

% Continue: Maximization Round 2 - Stocks rinit

<<maximization2-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(1234)
sd_02 = 2/3
starting_values_sbh_02 <- choose_n_fits(sbh_el_01, top_n_fits, ncores)

bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_02/mifs_global_sbh_02.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_01[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_02, beta2 = 0.01*sd_02, beta3 = 0.01*sd_02,
        beta11 = 0.01*sd_02, phi = 0.01*sd_02,
        sigma_q = 0.01*sd_02, sigma_xi = 0.01*sd_02
      ),
      params = starting_values_sbh_02[[i]]
    ) 
  }
}) ->  mifs_global_sbh_02


bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_02/sbh_el_02.rds"),{
  eval_logLik_pomp(mifs_global_sbh_02, ncores, NP_MIF, seed=4248930)
}) -> sbh_el_02
@

% Continue: Maximization Round 3 - Stocks rinit

<<maximization3-Stocks_rinit, echo=F, warning=F, message=F>>=
set.seed(12345)
sd_03 = (2/3)^2
starting_values_sbh_03 <- choose_n_fits(sbh_el_02, top_n_fits, ncores)

bake(file = paste0(out_dir, "mif2_maximization/SBH_init/round_03/mifs_global_sbh_03.rds"),{ 
  plan(multicore, workers = ncores)
  foreach(i = 1:ncores, .options.future = list(seed = 652643293)) %dofuture% {
    pomp::mif2(
      mifs_global_sbh_02[[1]],
      rw.sd = rw_sd(
        beta1 = 0.01*sd_03, beta2 = 0.01*sd_03, beta3 = 0.01*sd_03,
        beta11 = 0.01*sd_03, phi = 0.01*sd_03,
        sigma_q = 0.01*sd_03, sigma_xi = 0.01*sd_03
      ),
      params = starting_values_sbh_03[[i]]
    ) 
  }
}) -> mifs_global_sbh_03


bake(file = paste0(out_dir,"mif2_maximization/SBH_init/round_03/sbh_el_03.rds"),{
  eval_logLik_pomp(mifs_global_sbh_03, ncores, NP_MIF, seed = 4248930)
}) ->  sbh_el_03
@

<<estimated-mle_stocks_init, echo=FALSE, message=FALSE, warning=FALSE>>=
# Can be changed if more rounds needed
est_mle_vec_sbh <- sbh_el_03[which.max(sbh_el_03$logLik),]
@



%%%%%%%%%% 11111111 %%%%%%%%%%%%

\section{Introduction}

This article results from an investigation of the results presented by \citet{wwr} (henceforth, WWR) in their Table~1.
WWR were given the opportunity to submit a correction, after we shared the results of our investigation with them, but they declined.
The theory developed by WWR shows that PAL has some potentially useful scaling properties, but the numerical results in Table~1 appear to show much stronger performance than a standard particle filter on an example of scientific interest but moderate size.
Our task here is to correct the error in Table~1 so that researchers considering whether to implement Poisson approximate likelihood (PAL) are appropriately informed about its benefits.

Table~1 of WWR reanalyzes the model and data of \citet{stocks} (henceforth, SBH) for which the likelihood was calculated using a particle filter.
SBH found strong evidence for the importance of overdispersion in a stochastic dynamic model for their epidemiological data.
This is significant because earlier research on population dynamics largely avoided consideration of overdispersion, perhaps because of the lack of avaialable statistical methodology to fit such models. 
The conclusions of SBH hinge on a comparison of likelihoods, and so the results of WWR discredit those conclusions by indicating that SBH based their reasoning on inaccurately computed likelihoods.
An important consequence of correcting Table~1 of WWR is that the results of SBH stand undiminished. 

SBH and WWR each fitted three different rotavirus models.
The first has equidispersion (i.e., no overdispersion) in the measurement model and the dynamic model, and is called EqEq.
The second, EqOv, includes overdispersion in only the measurement model.
The third, OvOv, includes overdispersion in both these model components.
We focus on OvOv, which WWR found to be the best fitting model. 

We show that most of the apparent advantage for PAL on the OvOv model, compared to a particle filter, arises because WWR used a different scaling of the data from SBH.
Two models for the same data can properly be compared by their likelihood, even if the models have entirely different structures.
One can make allowance for the number of estimated parameters using a quantity such as Akaike's information criterion \citep{Akaike1974ANL}.
However, if data are rescaled, a correction is required to make likelihoods comparable.
For example, if one model describes a dataset in grams and another describes it in kilograms, then the latter model will earn an increased log-likelihood of $\log(10^3)$ for each data point simply because of the change in scale.
Presenting a direct comparison of a likelihood for the data in grams with a likelihood for the data in kilograms would evidently be inappropriate.


<<sim-100, echo=F, warning=F, message=F>>=
bake(file = paste0(out_dir, "pfilter/simulated_dataset/ovov_sim_100.rds"),{
  ovov_sim_computation <- list()
  i <- 1
  repeat {
    # Simulation
    sim_data <- simulate(sir_wwr, format = "data.frame") |>
      subset(select=c("time", "cases1", "cases2", "cases3"))
    
    if (!any(sim_data == 0)) {
      ovov_sim_computation[[length(ovov_sim_computation) + 1]] <- sim_data
      i <- i + 1  
    } else {
      print(paste("Condition not met for iteration", i, "- retrying"))
    }
    
    if (length(ovov_sim_computation) >= sim_rep) break  
  }
  ovov_sim_computation
}) -> ovov_sim_100
@


<<pfilter_sim_100, echo=FALSE, message=FALSE, warning=FALSE, results='hide'>>=
bake(file = paste0(out_dir, "pfilter/simulated_dataset/pfilter_ovov_sim_100.rds"),{
  pfilter_obj <- list()
  pfilter_ovov_sim_computation <- c()
  
  for(i in 1:length(ovov_sim_100)){
    pomp(
      data = ovov_sim_100[[i]],
      times = "time",
      t0 = 0,
      dmeasure = dmeas,
      rmeasure = rmeas,
      rprocess = discrete_time(step.fun = rproc, delta.t = 1/4),
      statenames = statenames,
      paramnames = names(params_wwr),
      accumvars = c("H1", "H2", "H3"),
      rinit = rinit_wwr,
      partrans = pt_wwr,
      params = params_wwr
    )  -> pfilter_obj[[i]]
    
    plan(multicore, workers = ncores)
    
    foreach(j = 1:ncores, .combine = c,
      .options.future = list(seed = 998468235L)
    ) %dofuture% {
      pfilter(
        pfilter_obj[[i]], Np = NP_MIF, save.states = FALSE, params = params_wwr
      )
    } -> pfs
    pfilter_ovov_sim_computation[i] <- logmeanexp(logLik(pfs))  
  }
  pfilter_ovov_sim_computation
}) ->  pfilter_ovov_sim_100
@



<<palfilter_sim_100, echo=FALSE, message=FALSE, warning=FALSE, results='hide'>>=
prop <- 420
init_dist <- c(
  3876549, 30351, 1315221, 57139612, 871, 302852, 19573727, 2550, 131092
)
initial_guess <- c(11.48, 0.25, 0.35, 0.14, 0.16, 0.021, 66.89)

bake(file = paste0(out_dir, "pfilter/simulated_dataset/palfilter_ovov_sim_100.rds"),{
  plan(multicore, workers = ncores)
  foreach(
    i = 1:length(ovov_sim_100), .combine = c, .options.future = list(seed = 998468235L)
  ) %dofuture% {
    pal_sim_dat <- ovov_sim_100[[i]][,-1] |> as.matrix()
    lik_list <- rotavirus_SMC_qropxi(
      init_dist = init_dist, 
      y_obs = pal_sim_dat, 
      m = 9, 
      regular_params = c(
        initial_guess[1], initial_guess[2], initial_guess[3], initial_guess[4],
        initial_guess[5]
      ), 
      gamma_par = c(initial_guess[7], 1/initial_guess[7]), 
      norm_par = c(0.07, initial_guess[6]), 
      prop = t(prop), 
      n_particles = NP_MIF,
      ncores = ncores
    )
    lik_list$log_lik
  }
}) -> palfilter_ovov_sim_100

# lik_list$ll_storage |> sum()
@


<<pfilter_wwr, echo=FALSE, message=FALSE, warning=FALSE>>=
bake(file = paste0(out_dir, "pfilter/original_dataset/pfilter_wwr.rds"),{
  plan(multicore, workers = ncores)
  foreach(
    i = 1:ncores, .combine = c, .options.future = list(seed = 998468235L)
  ) %dofuture% {
      pfilter(sir_wwr, Np = NP_MIF, params = params_wwr)
    } 
}) ->  pfilter_wwr

bake(file = paste0(out_dir, "pfilter/original_dataset/pfilter_wwr_cond_lik.rds"),{
  pfilter_wwr_cond_lik_computation <- list()
  for(k in 1:ncores){
    pfilter_wwr_cond_lik_computation[[k]] <- pfilter_wwr[[k]]@cond.logLik
  }
  pfilter_wwr_cond_lik_computation
}) -> pfilter_wwr_cond_lik
@

<<palfilter_wwr, echo=FALSE>>=
realdat <- as.matrix(dat[,-1])

bake(file = paste0(out_dir,"pfilter/original_dataset/palfilter_lik_list_wwr.rds"),{
 set.seed(1223)
 rotavirus_SMC_qropxi(
    init_dist = init_dist, 
    y_obs = realdat, 
    m = 9, 
    regular_params = c(
      initial_guess[1], initial_guess[2], initial_guess[3], initial_guess[4],
      initial_guess[5]
    ), 
    gamma_par = c(initial_guess[7], 1/initial_guess[7]), 
    norm_par = c(0.07, initial_guess[6]), 
    prop = t(prop), 
    n_particles = NP_MIF, 
    ncores = ncores
  ) 
}) -> palfilter_lik_list_wwr
@

<<PAL-MLE-from-mif2, echo=FALSE, message=FALSE, warning=FALSE>>=
prop <- 420
realdat <- as.matrix(dat[,-1])
init_dist_mif2_mle <- c(
  pop*as.numeric(est_mle_vec["S10"]), 
  pop*as.numeric(est_mle_vec["I10"]), 
  pop*as.numeric(est_mle_vec["R10"]),
  pop*as.numeric(est_mle_vec["S20"]),
  pop*as.numeric(est_mle_vec["R20"]),
  pop*as.numeric(est_mle_vec["I20"]),
  pop*as.numeric(est_mle_vec["S30"]),
  pop*as.numeric(est_mle_vec["R30"]),
  pop*as.numeric(est_mle_vec["I30"])
)

initial_guess_mif2_mle <- c(
  as.numeric(est_mle_vec["beta1"]), 
  as.numeric(est_mle_vec["beta2"]), 
  as.numeric(est_mle_vec["beta3"]),
  as.numeric(est_mle_vec["phi"]),
  as.numeric(est_mle_vec["beta11"]),
  as.numeric(est_mle_vec["sigma_q"]),
  as.numeric(est_mle_vec["sigma_xi"])
)

bake(file = paste0(out_dir,"pfilter/original_dataset/palfilter_mif2_mle.rds"),{
 set.seed(1223)
 rotavirus_SMC_qropxi(
    init_dist = init_dist_mif2_mle, 
    y_obs = realdat, 
    m = 9, 
    regular_params = c(
      initial_guess_mif2_mle[1], 
      initial_guess_mif2_mle[2], 
      initial_guess_mif2_mle[3], 
      initial_guess_mif2_mle[4],
      initial_guess_mif2_mle[5]
    ), 
    gamma_par = c(initial_guess_mif2_mle[7], 1/initial_guess_mif2_mle[7]), 
    norm_par = c(0.07, initial_guess_mif2_mle[6]), 
    prop = t(prop), 
    n_particles = NP_MIF, 
    ncores = ncores
  ) 
}) -> palfilter_mif2_mle
@

\begin{table}[ht] % The placement specifier can be [h!tbp]
\centering % Centers the table
\caption{AIC for the OvOv rotavirus model, computed using two filtering methods. PAL is the Poisson approximate likelihood, implemented using the code of WWR. PF is the particle filter, implemented using the code of \citet{pomppackagepaper}. The first two lines are from WWR, Table~1, and lines 3--5 are our own computations. We used 50000 particle for both PF and PAL. PF was repeated 36 times to reduce the Monte Carlo variance, but this step was not necessary for PAL.
\ed{SHOULD WE HAVE A LINE FOR PAL AT THE IF2 MLE FROM LINE 5? PERHAPS WE CAN'T READILY MAXIMIZE PAL FOR THIS MODEL (OR CAN WE?) BUT SOME SORT OF COMPARISON WOULD BE HELPFUL}
} 
\label{tab:ovovrealdata} % For referencing this table elsewhere in your text
\begin{tabular}{lllc} 
  \hline
  & Method & Result & AIC  \\
  \hline
  1. & PAL & Table~1 of WWR & 13778.08
  \\
  2. & PF & Table~1 of WWR, originally from SBH &  20134.38
  \\
  3. & PAL & Model, data and MLE from WWR &
    \Sexpr{myround(-2*palfilter_lik_list_wwr$log_lik + 2*7 ,2)} 
  \\
  4. & PF & Model, data and MLE from WWR &
    \Sexpr{myround(-2*logmeanexp(logLik(pfilter_wwr)+ 2*7),2)}
  \\
  5. & PF & Data from WWR, estimated initial values &
     \Sexpr{myround(2*((length(params_wwr) - length(params_fixed)) - est_mle_vec["logLik"]),2)}
  \\
  6. & PAL & Data from WWR, estimated initial values &
     \Sexpr{myround(-2*palfilter_mif2_mle$log_lik + 2*7 ,2)}
  \\
 \hline
\end{tabular}
\end{table}
 
SBH fitted their model to a dataset derived by dividing the original reported count data by an estimated reporting rate, to put their data on the scale of the actual number of cases in the population, whereas WWR fitted directly to the report data.
The reporting rate used by SBH varied over time, but was generally around $7\%$.
On approximately 1200 data points, this corresponds to a discrepancy of around $-1200\log(0.07) \approx 3200$ log-likelihood units, largely explaining the difference reported in Table~1 and interpreted by WWR as evidence supporting PAL.
The comparison can be corrected either by applying the method of SBH to the data of WWR or vice versa.
Since the method of SBH is applicable to a more general class of models, and supported by published software, it was convenient to apply the SBH method to the model and data of WWR.
The large discrepancy in log-likelihood disappears at this point by recomputing the likelihood of the model using PAL and particle filter separately (see Table~\ref{tab:ovovrealdata}).
This re-analysis does, however, show a discrepancy between two methods.
We continued our investigation to establish the cause of this.
We discovered that the initial conditions for the latent process in January 2001 (their time 0) were estimated by SBH. However, in WWR, they were assumed fixed and made the model fitted poorly. 


<<plot2-cond-loglik, echo=FALSE, fig.height=4, fig.width=5.5, out.width="4.5in",fig.align="center", fig.cap='Conditional log-likelihoods computed using PAL (red) and PF (blue) for the rotavirus model and MLE from WWR. We used 50000 particle for both PF and PAL. PF averaged over 36 replications to reduce the Monte Carlo variance, but this step was not necessary for PAL. The main sources of likelihood shortfall of PF are at time points $t=1, 2, 3, 11, 81, 194$, and $325-333$.'>>=

# Combine the data into a single dataframe for SMC with grouping variable
data_list <- lapply(1:ncores, function(i) {
  data.frame(
    time = time, 
    cond_loglik = pfilter_wwr_cond_lik[[i]], 
    method = "SMC",
    group = i
  )
})

# Combine all the SMC data frames into one
df_smc <- bind_rows(data_list)

# Create the PAL data frame
df_pal <- data.frame(
  time = time, 
  cond_loglik = palfilter_lik_list_wwr$ll_storage, 
  method = "PAL",
  group = 1  # Single group for PAL
)

# Combine both data frames
df <- bind_rows(df_smc, df_pal)

# Plot with ggplot
ggplot(df, aes(x = time, y = cond_loglik, color = method)) +
  geom_line(data = df %>% filter(method == "SMC"), aes(group = group), color = "blue", alpha = 0.7) +
  geom_line(data = df %>% filter(method == "PAL"), aes(group = group), color = "red") +
  theme_minimal() +
  labs(
    title = NULL,
    x = "Time (weeks)",
    y = "Conditional Log-likelihood",
    color = "Method"
  ) +
  theme(legend.position = "bottom")
@



\begin{table}[htbp]
    \caption{Maximum likelihood estimation of parameters for the OvOv model of WWR by PAL (taken from WWR), and by iterated particle filtering implemented using {\rm \texttt{mif2}} in {\rm \texttt{pomp}} \citep{pomppackagepaper}. The initial distribution parameter \(\lambda_0 = (S_{10}, I_{10}, R_{10}, S_{20}, I_{20}, R_{20}, S_{30}, I_{30}, R_{30})\) are assumed to be fixed in \cite{wwr}. The iterated particle filtering estimates were obtained by using 3 rounds of refinement, with 100 filtering iterations, 50,000 particles, and 36 replicates in each round with the top 12 best fits in terms of likelihood chosen to be the starting value for the next round.}
    \label{tab:mlebywwr}
\begin{center}    
    \begin{tabular}{lccc}  % Left-aligned for the first column, centered for the other two columns
      \toprule
      Parameter      & PAL      & iterated filtering & iterated filtering (stocks)\\
      \midrule
      \(\beta_1\)    & 11.48     & $\Sexpr{round(est_mle_vec["beta1"],2)}$ & $\Sexpr{round(est_mle_vec_sbh["beta1"],2)}$\\
      \(\beta_2\)    & 0.25      & $\Sexpr{round(est_mle_vec["beta2"],2)}$ & $\Sexpr{round(est_mle_vec_sbh["beta2"],2)}$\\
      \(\beta_3\)    & 0.35      & $\Sexpr{round(est_mle_vec["beta3"],2)}$ & $\Sexpr{round(est_mle_vec_sbh["beta3"],2)}$\\
      \(\phi\)       & 0.14      & $\Sexpr{round(est_mle_vec["phi"],2)}$   & $\Sexpr{round(est_mle_vec_sbh["phi"],2)}$\\
      \(\rho\)       & 0.16      & $\Sexpr{round(est_mle_vec["beta11"],2)}$ & $\Sexpr{round(est_mle_vec_sbh["beta11"],2)}$\\
      \(\sigma^2_q\) & 0.021     & $\Sexpr{round(est_mle_vec["sigma_q"],3)}$ & $\Sexpr{round(est_mle_vec_sbh["sigma_q"],3)}$\\
      \(\sigma_\xi\) & 66.89     & $\Sexpr{round(est_mle_vec["sigma_xi"],2)}$ & $\Sexpr{round(est_mle_vec["sigma_xi"],2)}$\\
      \(S_{10}\)     & 3876549   & $\Sexpr{round(pop*est_mle_vec["S10"],0)}$  & $5094840$\\
      \(I_{10}\)     & 30351     & $\Sexpr{round(pop*est_mle_vec["I10"],0)}$  & $2871$\\
      \(R_{10}\)     & 1315221   & $\Sexpr{round(pop*est_mle_vec["R10"],0)}$  & $124410$\\
      \(S_{20}\)     & 57139612  & $\Sexpr{round(pop*est_mle_vec["S20"],0)}$  & $57385600$\\
      \(I_{20}\)     & 871       & $\Sexpr{round(pop*est_mle_vec["I20"],0)}$  & $639$\\
      \(R_{20}\)     & 302852    & $\Sexpr{round(pop*est_mle_vec["R20"],0)}$  & $57072$\\
      \(S_{30}\)     & 19573727  & $\Sexpr{round(pop*est_mle_vec["S30"],0)}$  & $19697600$\\
      \(I_{30}\)     & 2550      & $\Sexpr{round(pop*est_mle_vec["I30"],0)}$  & $174$\\
      \(R_{30}\)     & 131092    & $\Sexpr{round(pop*est_mle_vec["R30"],0)}$  & $9578$\\
      \midrule
      \textbf{AIC}  & 13778.08  & $\Sexpr{round(2*((length(params_wwr) - length(params_fixed)) - est_mle_vec["logLik"]),2)}$ & $\Sexpr{round(2*((length(params_wwr) - length(params_fixed)) - est_mle_vec_sbh["logLik"]),2)}$\\
      \bottomrule
    \end{tabular}
\end{center}
\end{table}


<<plot1-100-sim-compare,echo=FALSE, fig.height=4, fig.width=5.5, out.width="4.5in",fig.align="center",fig.cap='Log-likelihoods computed using two filtering methods for 100 randomly simulated datasets at the MLE of the OvOv model. Simulations with one or more zero counts were disqualified since they resulted in errors for the PAL implementation. We used 50000 particle for both PF and PAL. PF was repeated 36 times to reduce the Monte Carlo variance, but this step was not necessary for PAL. The red line corresponds to equality of the two estimates.'>>=
data <- data.frame(
  pfilter = pfilter_ovov_sim_100, 
  palfilter = palfilter_ovov_sim_100
)

ggplot(data, aes(x = pfilter, y = palfilter)) +
geom_point(shape = 20, size = 0.3) +  # pch=20, cex=0.3 equivalent
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(
  title=NULL,
  x = "PF",
  y = "PAL"
) +
theme_minimal()
@

Figure~\ref{fig:plot2-cond-loglik} indicates that the worst conditional log-likelihood values, estimated using the particle filter, arise at the start of the time series.
This could be a problem with data collection, or some subtle issue with the science of the epidemiological system, but a simple possibility is that the initial values of the latent state process might be poorly specified.
For this model, the initial values were fixed based on scientific reasoning, rather than being estimated.
If we instead estimate the initial values, we obtain the results in Table~\ref{tab:mlebywwr}. 

\ed{SINCE THE MEAN DURATION OF IMMUNITY IS 1 YR, HAVING $31\times 10^6$ in $R_{2,0}$ IMPLIES THAT ABOUT HALF OF ALL SUSCEPTIBLE INDIVIDUALS GET ROTAVIRUS EVERY YEAR ACCORDING TO THE MODEL. IS THAT THE CASE FOR THE MODEL? FOR THE DATA, THE PER-CAPITA RATE IN THIS AGE GROUP IS RATER LOW. THE SIMPLEST WAY TO AVOID THIS PROBLEM WOULD BE TO DO WHAT SBH DID (DESCRIBED IN THEIR ONLINE APPENDIX C) - EFFECTIVELY, DRAWING $X_0$ FROM THE STATIONARY DISTRIBUTION GIVEN THE MODEL. THIS HAS THE ADVANTAGE OF INCLUDING NO NEW PARAMETERS AND BEING CONSISTENT WITH SBH. WE DON'T KNOW IN ADVANCE EXACTLY HOW SUCCESSFUL IT WOULD BE...}

\section*{Discussion}

PAL provides an approximate filter, whereas the particle filter provides a Monte~Carlo estimate of an exact filter.
Possible explanations for PAL obtaining a higher log-likelihood than a particle filter include:
\begin{enumerate}
\item The particle filter could have high Monte Carlo variance. This results in negative bias in its log-likilihood estimate due to Jensen's inequality, since the particle filter provides an unbiased likelihood estimate.
\item If the model is misspecified, the approximation error in PAL could lead to a higher log-likelihood estimate than that of the actual misspecified model.
\end{enumerate}
Log-likelihood is a proper scoring rule for forecasts \cite{gneiting2}, and both the particle filter and PAL construct their log-likelihood estimates via a sequence of one-step forecasts.
Therefore, if the model is correctly specified, the approximation error in PAL can only decrease the expected log-likelihood.
We tested this on simulated data, for which the model of \cite{wwr} is correctly specified.
For this simulation study, the particle filter out-performs PAL (Figure~\ref{fig:plot1-100-sim-compare}) which supports hypothesis 2, above.
On average, the particle filter likelihood estimate is $\Sexpr{myround(mean(pfilter_ovov_sim_100)-mean(palfilter_ovov_sim_100),1)}$ log units higher than the PAL.
Consequently, we looked for model misspecification by comparing the conditional log-likelihood estimates at each time point, as described in the Methods section.

We conclude that PAL is a potentially useful algorithm, with some favorable theoretical properties.
However, the corrected evidence does not indicate an advantage for using PAL in situations where the particle filter is effective.

%%%%%%%%% 222222222 %%%%%%%%%%

\section*{Methods}

We first replicated the model of WWR in the R package \texttt{pomp} \citep{pomppackagepaper}.
Comparing summary statistics of simulations confirms that both model versions are essentially identical, as described by \citet{hao24}.
We therefore implement PAL using the code from \citet{wwr}, comparing directly with the particle filter implemented using \texttt{pomp}.

This article has focused on likelihood evaluation, but the inferences presented also require the likelihood estimate to be maximized.
WWR used direct numerical maximization, whereas SBH used iterated filtering to maximize the particle filter likelihood estimate implemented using the \texttt{mif2} algorithm in \texttt{pomp}.
We follow the approaches adopted by these two papers.
The optimization method is not directly pertinent to the comparison of the filtering methods, so we do not discuss optimization in further detail here.
An extended description of our reanalysis of WWR is provided by \cite{hao24}.
That thesis also contains some additional results reinforcing the conclusions presented in this article.
The source code for this article is available at \url{https://github.com:ionides/pal-vs-pf} and archived at \ed{ZENODO - TODO}.

\bibliography{bib-pal}

\end{document}
